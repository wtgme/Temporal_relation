{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b089890b-8dc0-4333-8b76-fcce824317fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xmltodict, json\n",
    "import os\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from itertools import combinations\n",
    "import json, re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ea9dd3-6fc4-42ac-96b0-af9954baab60",
   "metadata": {},
   "source": [
    "### Load raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ec8452a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pwd\n",
    "# !conda env list\n",
    "# !python --version\n",
    "# !cd /home/jovyan/work/Temporal_relation/\n",
    "# !pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ceeee538-9c9a-4043-8750-f04b4a692afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to path to the data\n",
    "# path = '/home/wt/Downloads/n2c2 2012/'\n",
    "wp = '/home/jovyan/work/Temporal_relation/'\n",
    "path = wp + 'data/i2b2/'\n",
    "training_data_path = path + 'merge_training'\n",
    "test_data_path = path + 'ground_truth/merged_xml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03b04a12-9b35-446c-8984-8dffb38b6575",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(data_path):\n",
    "    data = {}\n",
    "    for filename in os.listdir(data_path):\n",
    "        if filename.endswith(\".xml\"): \n",
    "            f = (os.path.join(data_path, filename))\n",
    "#             print(f)\n",
    "            fb = open(f, \"rb\").read().decode(encoding=\"utf-8\")\n",
    "#     invalid character '&' https://github.com/martinblech/xmltodict/issues/277\n",
    "            fb = fb.replace('&', '&amp;')\n",
    "            dic = xmltodict.parse(fb, attr_prefix='')\n",
    "#     restore orginal character \"&\"\n",
    "            dic['ClinicalNarrativeTemporalAnnotation']['TEXT'] = dic['ClinicalNarrativeTemporalAnnotation']['TEXT'].replace('&amp;', '&')\n",
    "            data[filename] = (dic)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58df447f-02f8-4e82-a7cc-e48cf806524e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data_loader(training_data_path)\n",
    "test_data = data_loader(test_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f519491c-a84d-4f5f-a9f9-1c2fcbd18ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190 120\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data), len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6fdb414-c896-447c-927b-8cb13664e9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_first_regex(text, substrings):\n",
    "    pattern = '|'.join(map(re.escape, substrings))  # Escape special characters\n",
    "    match = re.search(pattern, text)\n",
    "    if match:\n",
    "        return match.start()\n",
    "    else:\n",
    "        raise ValueError(\"None of the substrings found in the text.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c5928d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_section_graph(doc_id, data, section='all'):\n",
    "    # for doc_id in list(data.keys())[:1]:\n",
    "    text = data[doc_id]['ClinicalNarrativeTemporalAnnotation']['TEXT']\n",
    "    # print(text)\n",
    "    \n",
    "    history_start = text.index('HISTORY OF PRESENT ILLNESS ')\n",
    "    substrings = ['HOSPITAL COURSE']\n",
    "    history_end = find_first_regex(text, substrings)\n",
    "\n",
    "    sect_start, sect_end = 0, len(text)\n",
    "    if section == 'history':\n",
    "        sect_start, sect_end = history_start, history_end\n",
    "    elif section == 'other':\n",
    "        sect_start, sect_end = history_end, len(text)\n",
    "    # print(text[sect_start:sect_end])\n",
    "\n",
    "    events = pd.DataFrame(data[doc_id]['ClinicalNarrativeTemporalAnnotation']['TAGS']['EVENT'])\n",
    "    events['start'] = events['start'].astype(int)\n",
    "    events['end'] = events['end'].astype(int)\n",
    "    # Filter events in the history section\n",
    "    # print('events', events.shape)\n",
    "    events = events.loc[(events['start']>=sect_start) & (events['end']<=sect_end)]\n",
    "    # print('events after', events.shape)\n",
    "    \n",
    "    # FILTER 1: only use events related to medical concepts\n",
    "    # events = events.loc[events['type'].isin(['PROBLEM', 'TEST', 'TREATMENT'])]\n",
    "    event_types = dict(zip(events['id'], events['type']))\n",
    "    \n",
    "    # Remove duplicated admission and discharge time.\n",
    "    # adm_dis = pd.DataFrame(data[doc_id]['ClinicalNarrativeTemporalAnnotation']['TAGS']['SECTIME'])\n",
    "    times = pd.DataFrame(data[doc_id]['ClinicalNarrativeTemporalAnnotation']['TAGS']['TIMEX3'])\n",
    "    times['start'] = times['start'].astype(int)\n",
    "    times['end'] = times['end'].astype(int)\n",
    "    # print('times', times.shape)\n",
    "    if section == 'history':\n",
    "        times = times.loc[((times['start']>=sect_start) & (times['end']<=sect_end)) | (times['id']=='T0')]\n",
    "    elif section == 'other':\n",
    "        times = times.loc[((times['start']>=sect_start) & (times['end']<=sect_end))| (times['id']=='T1')]\n",
    "    else:\n",
    "        times = times.loc[((times['start']>=sect_start) & (times['end']<=sect_end))| (times['id'].isin(['T0', 'T1']))]\n",
    "    # print('times after', times.shape)\n",
    "    time_types = dict(zip(times['id'], times['type']))\n",
    "    \n",
    "    nodes_keep = list(events['id']) + list(times['id'])\n",
    "    # print(len(nodes_keep))\n",
    "    \n",
    "    all_links = pd.DataFrame(data[doc_id]['ClinicalNarrativeTemporalAnnotation']['TAGS']['TLINK'])\n",
    "    all_links = all_links.loc[all_links['type']!='']\n",
    "\n",
    "    # links = all_links.loc[(all_links['id'].str.lower().str.contains('sectime')==False)]\n",
    "    \n",
    "    # FILTER 2: Exclude sectime links not about admission\n",
    "    if section == 'history':\n",
    "        # section_links = all_links.loc[(all_links['id'].str.lower().str.contains('sectime')==True) & (all_links['toID']=='T0')]\n",
    "        links = all_links.loc[(all_links['id'].str.lower().str.contains('sectime')==False) | ((all_links['id'].str.lower().str.contains('sectime')==True) & (all_links['toID']=='T0'))]\n",
    "    elif section == 'other':\n",
    "        # section_links = all_links.loc[(all_links['id'].str.lower().str.contains('sectime')==True) & (all_links['toID']=='T1')]\n",
    "        links = all_links.loc[(all_links['id'].str.lower().str.contains('sectime')==False) | ((all_links['id'].str.lower().str.contains('sectime')==True) & (all_links['toID']=='T1'))]\n",
    "    else:\n",
    "        links = all_links\n",
    "    # print(section_links.shape)\n",
    "    # print(section_links.head())\n",
    "    # print(section_links.groupby('type')['fromID'].unique())\n",
    "    # print(set(section_links['fromID']) - set(nodes_keep))\n",
    "    # print(set(nodes_keep) -  set(section_links['fromID']))\n",
    "    # if section != 'all':\n",
    "    #     node_category = dict(zip(section_links['fromID'], section_links['type']))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Normalize AFTER and BEFORE relations\n",
    "    links = links.copy()\n",
    "    mask = (links['type'] == 'AFTER')\n",
    "    links.loc[mask, ['fromID', 'fromText', 'toID', 'toText']] = links.loc[mask, ['toID', 'toText', 'fromID', 'fromText']].values\n",
    "    links.loc[mask, 'type'] = 'BEFORE'\n",
    "    links = links.drop_duplicates(subset=['fromID', 'fromText', 'toID', 'toText', 'type'], keep='last')\n",
    "    \n",
    "    \n",
    "    G = nx.from_pandas_edgelist(links[['fromID', 'toID', 'type']], source='fromID', target='toID', edge_attr=True, create_using=nx.DiGraph())\n",
    "    source_nodes = dict(zip(links['fromID'], links['fromText']))\n",
    "    target_nodes = dict(zip(links['toID'], links['toText']))\n",
    "    nx.set_node_attributes(G, source_nodes|target_nodes, 'text')\n",
    "    # if section != 'all':\n",
    "    #     nx.set_node_attributes(G, node_category, 'time2section')\n",
    "    nx.set_node_attributes(G, event_types|time_types, 'type')\n",
    "    \n",
    "    # only keep nodes of interest\n",
    "    # FILTER 3: only subgraph\n",
    "    G = G.subgraph(nodes_keep).copy()\n",
    "    \n",
    "    # clear reverse links and reduce redundent nodes; \n",
    "    # There are no many duplicated links\n",
    "    return G, text[sect_start:sect_end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "a17798a3-14b1-42ac-bb25-b2261929447a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All nodes and links in the data;\n",
    "# G, text = build_section_graph('36.xml', train_data, 'all')\n",
    "# Only nodes and links in the history section\n",
    "G, history = build_section_graph('36.xml', train_data, 'history')\n",
    "# Only nodes and links in sections other than \"history\" section\n",
    "# G, text = build_section_graph('36.xml', train_data, 'other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "7e9a079e-801b-45a8-aab9-736669887c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph with 56 nodes and 99 edges\n"
     ]
    }
   ],
   "source": [
    "print(f\"Graph with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "571334a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of weakly connected components: 1\n",
      "Component sizes: [56]\n"
     ]
    }
   ],
   "source": [
    "num_components = nx.number_weakly_connected_components(G)\n",
    "print(f\"Number of weakly connected components: {num_components}\")\n",
    "\n",
    "weakly_connected = list(nx.weakly_connected_components(G))\n",
    "print(f\"Component sizes: {[len(comp) for comp in weakly_connected]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "7bac7149-1cac-40e6-b5ed-595b1faa4c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nx.write_graphml(G, wp+\"graphs/tem_history_graph.graphml\")\n",
    "# nx.write_graphml(G, wp+\"graphs/tem_hospital_graph.graphml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bad22b-d17c-406f-87ca-4844dfa2500b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f3b70aa-49ae-4894-b96d-3947694c9f8d",
   "metadata": {},
   "source": [
    "### Merge overlap nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "4c5c0909-72d8-496a-8a9b-5a0b5597aa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_overlapping_nodes(G):\n",
    "    \"\"\"\n",
    "    Merge nodes that are connected by edges with type='overlp'.\n",
    "    Only merges edges across merged groups if they have the same direction.\n",
    "    Maintains node attributes and edge attributes as JSON strings for GraphML compatibility.\n",
    "    Tracks source nodes for each merged edge.\n",
    "    \n",
    "    Args:\n",
    "        G (nx.Graph or nx.DiGraph): Input graph\n",
    "        \n",
    "    Returns:\n",
    "        nx.Graph or nx.DiGraph: New graph with merged nodes and edges\n",
    "    \"\"\"\n",
    "    # Create new merged graph of same type as input\n",
    "    merged_G = G.__class__()\n",
    "    \n",
    "    # Find connected components considering only overlap edges\n",
    "    overlap_edges = [(u, v) for u, v, d in G.edges(data=True) if d.get('type') == 'OVERLAP']\n",
    "    overlap_graph = nx.Graph()  # Undirected for finding components\n",
    "    overlap_graph.add_edges_from(overlap_edges)\n",
    "    \n",
    "    # Get clusters of nodes to merge\n",
    "    clusters = list(nx.connected_components(overlap_graph))\n",
    "    \n",
    "    # Create mapping from original nodes to their merged cluster names\n",
    "    node_to_cluster = {}\n",
    "    for cluster in clusters:\n",
    "        cluster = list(cluster)\n",
    "        merged_name = '+'.join(sorted(cluster))\n",
    "        for node in cluster:\n",
    "            node_to_cluster[node] = merged_name\n",
    "    \n",
    "    # Process nodes\n",
    "    for cluster in clusters:\n",
    "        cluster = list(cluster)\n",
    "        \n",
    "        if len(cluster) == 1:\n",
    "            # Single node, just copy it and its attributes\n",
    "            node = cluster[0]\n",
    "            merged_G.add_node(node, **G.nodes[node])\n",
    "            continue\n",
    "            \n",
    "        # Create merged node name\n",
    "        merged_node = '+'.join(sorted(cluster))\n",
    "        \n",
    "        # Combine node attributes and convert to JSON string\n",
    "        merged_attrs = {\n",
    "            'original_nodes': json.dumps(cluster),\n",
    "            'node_attributes': json.dumps({node: dict(G.nodes[node]) for node in cluster})\n",
    "        }\n",
    "        \n",
    "        # Add merged node\n",
    "        merged_G.add_node(merged_node, **merged_attrs)\n",
    "    \n",
    "    # Add nodes that weren't in any cluster\n",
    "    unclustered_nodes = set(G.nodes()) - set(node for cluster in clusters for node in cluster)\n",
    "    for node in unclustered_nodes:\n",
    "        merged_G.add_node(node, **G.nodes[node])\n",
    "        node_to_cluster[node] = node  # Map to itself\n",
    "    \n",
    "    # Create a dictionary to store edges between clusters\n",
    "    cluster_edges = {}  # (from_cluster, to_cluster) -> list of original edges with source info\n",
    "    \n",
    "    # Process edges\n",
    "    for u, v, data in G.edges(data=True):\n",
    "        # Get cluster names (or original names for unclustered nodes)\n",
    "        u_cluster = node_to_cluster[u]\n",
    "        v_cluster = node_to_cluster[v]\n",
    "        \n",
    "        # Skip internal edges of merged clusters if they were overlap edges\n",
    "        if u_cluster == v_cluster and data.get('type') == 'OVERLAP':\n",
    "            continue\n",
    "        \n",
    "        # Add source node information to edge data\n",
    "        edge_data = data.copy()\n",
    "        edge_data['source_nodes'] = {'from': u, 'to': v}\n",
    "        \n",
    "        # Create edge key based on direction\n",
    "        edge_key = (u_cluster, v_cluster)\n",
    "        \n",
    "        # For directed graphs, maintain direction information\n",
    "        if isinstance(G, nx.DiGraph):\n",
    "            if edge_key not in cluster_edges:\n",
    "                cluster_edges[edge_key] = []\n",
    "            cluster_edges[edge_key].append((u, v, edge_data))\n",
    "        else:\n",
    "            # For undirected graphs, normalize the edge key\n",
    "            normalized_key = tuple(sorted([u_cluster, v_cluster]))\n",
    "            if normalized_key not in cluster_edges:\n",
    "                cluster_edges[normalized_key] = []\n",
    "            cluster_edges[normalized_key].append((u, v, edge_data))\n",
    "    \n",
    "    # Add merged edges to the graph\n",
    "    for (from_cluster, to_cluster), edges in cluster_edges.items():\n",
    "        # For directed graphs, check if all edges have the same direction\n",
    "        if isinstance(G, nx.DiGraph):\n",
    "            # Check if all edges go in the same direction\n",
    "            directions = set((u_cluster, v_cluster) \n",
    "                           for u, v, _ in edges\n",
    "                           for u_cluster, v_cluster in [(node_to_cluster[u], node_to_cluster[v])])\n",
    "            \n",
    "            if len(directions) == 1:  # All edges have same direction\n",
    "                merged_G.add_edge(from_cluster, to_cluster, \n",
    "                                edge_attributes=json.dumps([data for _, _, data in edges]))\n",
    "        else:\n",
    "            # For undirected graphs, just add the edge\n",
    "            merged_G.add_edge(from_cluster, to_cluster, \n",
    "                            edge_attributes=json.dumps([data for _, _, data in edges]))\n",
    "    \n",
    "    return merged_G\n",
    "\n",
    "def load_merged_graph(graphml_file):\n",
    "    \"\"\"\n",
    "    Load a merged graph from GraphML file and convert JSON string attributes back to Python objects.\n",
    "    \"\"\"\n",
    "    G = nx.read_graphml(graphml_file)\n",
    "    \n",
    "    # Convert node attributes back from JSON\n",
    "    for node in G.nodes():\n",
    "        if 'original_nodes' in G.nodes[node]:\n",
    "            G.nodes[node]['original_nodes'] = json.loads(G.nodes[node]['original_nodes'])\n",
    "            G.nodes[node]['node_attributes'] = json.loads(G.nodes[node]['node_attributes'])\n",
    "    \n",
    "    # Convert edge attributes back from JSON\n",
    "    for u, v in G.edges():\n",
    "        if 'edge_attributes' in G[u][v]:\n",
    "            G[u][v]['edge_attributes'] = json.loads(G[u][v]['edge_attributes'])\n",
    "    \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "68014572-7ea3-48de-9a7a-bec02d2c9cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mergaed_G = merge_overlapping_nodes(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "4e20d87b-48e0-49bf-ab5f-b7d66b66d521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph with 14 nodes and 20 edges\n"
     ]
    }
   ],
   "source": [
    "print(f\"Graph with {mergaed_G.number_of_nodes()} nodes and {mergaed_G.number_of_edges()} edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "ceb67fa0-b009-4592-92d8-d5f927263339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nx.write_graphml(mergaed_G, wp+\"graphs/tem_history_graph_merge.graphml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "0a9f53a9-a5cb-4643-9257-3de36fc85d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('E100+E82+E99', {'original_nodes': '[\"E99\", \"E100\", \"E82\"]', 'node_attributes': '{\"E99\": {\"text\": \"walker\", \"type\": \"TREATMENT\"}, \"E100\": {\"text\": \"a cane\", \"type\": \"TREATMENT\"}, \"E82\": {\"text\": \"ambulates\", \"type\": \"OCCURRENCE\"}}'}), ('E102+E105+E106+E111+E112+E113+E114+E115+E4+E5+E6+E7+E8+T4+T6+T7', {'original_nodes': '[\"E8\", \"E113\", \"E5\", \"E102\", \"E112\", \"T7\", \"E105\", \"E6\", \"E106\", \"T4\", \"E115\", \"T6\", \"E111\", \"E4\", \"E114\", \"E7\"]', 'node_attributes': '{\"E8\": {\"text\": \"syncope\", \"type\": \"PROBLEM\"}, \"E113\": {\"text\": \"lower extremity edema\", \"type\": \"PROBLEM\"}, \"E5\": {\"text\": \"sweating\", \"type\": \"PROBLEM\"}, \"E102\": {\"text\": \"short of breath\", \"type\": \"PROBLEM\"}, \"E112\": {\"text\": \"sharp\", \"type\": \"PROBLEM\"}, \"T7\": {\"text\": \"several years\", \"type\": \"DURATION\"}, \"E105\": {\"text\": \"Her shortness of breath\", \"type\": \"PROBLEM\"}, \"E6\": {\"text\": \"nausea\", \"type\": \"PROBLEM\"}, \"E106\": {\"text\": \"dyspnea\", \"type\": \"PROBLEM\"}, \"T4\": {\"text\": \"several years\", \"type\": \"DURATION\"}, \"E115\": {\"text\": \"Her lower extremity edema\", \"type\": \"PROBLEM\"}, \"T6\": {\"text\": \"a few seconds\", \"type\": \"DURATION\"}, \"E111\": {\"text\": \"nonradiating\", \"type\": \"PROBLEM\"}, \"E4\": {\"text\": \"chest twinges\", \"type\": \"PROBLEM\"}, \"E114\": {\"text\": \"cellulitis\", \"type\": \"PROBLEM\"}, \"E7\": {\"text\": \"vomiting\", \"type\": \"PROBLEM\"}}'}), ('E10+E11+E12+E13+E14+E15+E16+E17+E18+E19', {'original_nodes': '[\"E10\", \"E15\", \"E14\", \"E16\", \"E18\", \"E12\", \"E19\", \"E17\", \"E13\", \"E11\"]', 'node_attributes': '{\"E10\": {\"text\": \"headaches\", \"type\": \"PROBLEM\"}, \"E15\": {\"text\": \"hematuria\", \"type\": \"PROBLEM\"}, \"E14\": {\"text\": \"dyspnea\", \"type\": \"PROBLEM\"}, \"E16\": {\"text\": \"abdominal pain\", \"type\": \"PROBLEM\"}, \"E18\": {\"text\": \"melena\", \"type\": \"PROBLEM\"}, \"E12\": {\"text\": \"numbness\", \"type\": \"PROBLEM\"}, \"E19\": {\"text\": \"hematochezia\", \"type\": \"PROBLEM\"}, \"E17\": {\"text\": \"diarrhea\", \"type\": \"PROBLEM\"}, \"E13\": {\"text\": \"tingling\", \"type\": \"PROBLEM\"}, \"E11\": {\"text\": \"vision changes\", \"type\": \"PROBLEM\"}}'}), ('E103+E104', {'original_nodes': '[\"E103\", \"E104\"]', 'node_attributes': '{\"E103\": {\"text\": \"getting up from her chair\", \"type\": \"OCCURRENCE\"}, \"E104\": {\"text\": \"light headed\", \"type\": \"PROBLEM\"}}'}), ('E1+E107+E109+E110+E2+E3+E83+E95+E96+E97+E98+T2+T3+T5', {'original_nodes': '[\"E83\", \"E3\", \"T3\", \"E95\", \"E1\", \"E98\", \"T5\", \"E97\", \"E109\", \"E107\", \"E110\", \"E2\", \"T2\", \"E96\"]', 'node_attributes': '{\"E83\": {\"text\": \"presents\", \"type\": \"OCCURRENCE\"}, \"E3\": {\"text\": \"leg pain\", \"type\": \"PROBLEM\"}, \"T3\": {\"text\": \"2-3 years\", \"type\": \"DURATION\"}, \"E95\": {\"text\": \"increased shortness of breath\", \"type\": \"PROBLEM\"}, \"E1\": {\"text\": \"fevers\", \"type\": \"PROBLEM\"}, \"E98\": {\"text\": \"dyspnea\", \"type\": \"PROBLEM\"}, \"T5\": {\"text\": \"2 1/2 years\", \"type\": \"DURATION\"}, \"E97\": {\"text\": \"an associated dry cough\", \"type\": \"PROBLEM\"}, \"E109\": {\"text\": \"orthopnea\", \"type\": \"PROBLEM\"}, \"E107\": {\"text\": \"sleeps in a chair up right\", \"type\": \"OCCURRENCE\"}, \"E110\": {\"text\": \"noparoxysmal nocturnal dyspnea\", \"type\": \"PROBLEM\"}, \"E2\": {\"text\": \"chills\", \"type\": \"PROBLEM\"}, \"T2\": {\"text\": \"5 days\", \"type\": \"DURATION\"}, \"E96\": {\"text\": \"Her shortness of breath\", \"type\": \"PROBLEM\"}}'}), ('E93+E94', {'original_nodes': '[\"E94\", \"E93\"]', 'node_attributes': '{\"E94\": {\"text\": \"hypertension\", \"type\": \"PROBLEM\"}, \"E93\": {\"text\": \"obesity\", \"type\": \"PROBLEM\"}}'}), ('E116+T0', {'original_nodes': '[\"E116\", \"T0\"]', 'node_attributes': '{\"E116\": {\"text\": \"admission\", \"type\": \"OCCURRENCE\"}, \"T0\": {\"text\": \"02/01/2002\", \"type\": \"DATE\"}}'}), ('E101', {'text': 'osteoarthritis', 'type': 'PROBLEM'}), ('E119', {'text': 'denies', 'type': 'EVIDENTIAL'}), ('E118', {'text': 'a broken chair', 'type': 'OCCURRENCE'}), ('T8', {'text': 'the several weeks', 'type': 'DURATION'}), ('E108', {'text': 'osteoarthritis', 'type': 'PROBLEM'}), ('E117', {'text': 'an inability to elevate her legs', 'type': 'OCCURRENCE'}), ('E9', {'text': 'any pleural chest pain', 'type': 'PROBLEM'})]\n"
     ]
    }
   ],
   "source": [
    "print(list(mergaed_G.nodes(data=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02b554d-ba69-413d-a6cf-9755aaa5c841",
   "metadata": {},
   "source": [
    "### Remove conflicting relations (e.g., self-link and mutual links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "a799f672-1938-4398-9e6f-017630da4983",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_self_links(G):\n",
    "    H = G.copy()\n",
    "    self_loops = list(nx.selfloop_edges(H))\n",
    "    H.remove_edges_from(self_loops)\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "37831800-2134-45b6-8aac-5b67b35d2f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_mutual_links(G):\n",
    "    H = G.copy()\n",
    "    edges_to_remove = set()\n",
    "    \n",
    "    for u, v in G.edges():\n",
    "        if H.has_edge(v, u) and (v, u) not in edges_to_remove and (u, v) not in edges_to_remove:\n",
    "            edge1 = G.get_edge_data(u, v)\n",
    "            edge2 = G.get_edge_data(v, u)\n",
    "            edges_to_remove.add((u, v))\n",
    "            edges_to_remove.add((v, u))\n",
    "    H.remove_edges_from(edges_to_remove)\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "c393bd1b-f14b-476b-a1d2-1a37f6525f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mergaed_G_clean = remove_self_links(mergaed_G)\n",
    "mergaed_G_clean = remove_mutual_links(mergaed_G_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "59f526db-d5a7-4d8c-b172-ed057a9ea8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph with 14 nodes and 20 edges\n"
     ]
    }
   ],
   "source": [
    "print(f\"Graph with {mergaed_G_clean.number_of_nodes()} nodes and {mergaed_G_clean.number_of_edges()} edges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f653d8-94fc-44b6-83ea-07b65e5b951e",
   "metadata": {},
   "source": [
    "### Remove redundant links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "c1384a7b-ae17-4dd6-97aa-9a0ef1eebb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_redundant_edges(G):\n",
    "    redundant_edges = []\n",
    "    \n",
    "    # Iterate over edges while capturing their attributes\n",
    "    edges = list(G.edges(data=True))  # List of tuples (u, v, data_dict)\n",
    "    \n",
    "    for u, v, data in edges:\n",
    "        # Remove the edge and check if a path still exists\n",
    "        G.remove_edge(u, v)\n",
    "        \n",
    "        if nx.has_path(G, u, v):\n",
    "            redundant_edges.append((u, v))\n",
    "        \n",
    "        # Re-add the edge with its original attributes\n",
    "        G.add_edge(u, v, **data)\n",
    "    \n",
    "    # Remove redundant edges (preserves attributes of non-redundant edges)\n",
    "    G.remove_edges_from(redundant_edges)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "d6519493-50cd-4d39-9b13-a880c9c53ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mergaed_G_clean = remove_redundant_edges(mergaed_G_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "9806404d-6529-4659-851d-ce5f1dea5e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph with 14 nodes and 13 edges\n"
     ]
    }
   ],
   "source": [
    "print(f\"Graph with {mergaed_G_clean.number_of_nodes()} nodes and {mergaed_G_clean.number_of_edges()} edges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd70f989-585a-47cd-b7df-2714eb22844d",
   "metadata": {},
   "source": [
    "### Minimal paths and clean nodes rather than 'PROBLEM', 'TEST', 'TREATMENT'. A --> B --> C  and D-->C "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "ae72d5cf-46bf-4953-bc20-fbe015a5276e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimal_path_cover(G):\n",
    "    \"\"\"\n",
    "    Find a minimal collection of paths that cover all edges in a directed graph.\n",
    "    \n",
    "    Args:\n",
    "        G: A NetworkX directed graph (DiGraph)\n",
    "        \n",
    "    Returns:\n",
    "        A list of paths, where each path is a list of nodes\n",
    "    \"\"\"\n",
    "    if not G.edges():\n",
    "        return []\n",
    "    \n",
    "    # Create a working copy of the graph\n",
    "    remaining_edges = G.copy()\n",
    "    paths = []\n",
    "    \n",
    "    while remaining_edges.edges():\n",
    "        # Find longest path in the remaining graph\n",
    "        # This is a greedy approach - finding the truly minimal cover is NP-hard\n",
    "        longest_path = find_longest_path(remaining_edges)\n",
    "        \n",
    "        # Add the path to our collection\n",
    "        paths.append(longest_path)\n",
    "        \n",
    "        # Remove the edges in this path from the remaining graph\n",
    "        for i in range(len(longest_path) - 1):\n",
    "            u, v = longest_path[i], longest_path[i + 1]\n",
    "            if remaining_edges.has_edge(u, v):\n",
    "                remaining_edges.remove_edge(u, v)\n",
    "    \n",
    "    return paths\n",
    "\n",
    "def find_longest_path(G):\n",
    "    \"\"\"\n",
    "    Find the longest path in a directed graph.\n",
    "    \n",
    "    Args:\n",
    "        G: A NetworkX directed graph (DiGraph)\n",
    "        \n",
    "    Returns:\n",
    "        A list of nodes representing the longest path\n",
    "    \"\"\"\n",
    "    # For each node, try to find the longest path starting from it\n",
    "    longest_path = []\n",
    "    \n",
    "    for start_node in G.nodes():\n",
    "        # Skip nodes with no outgoing edges\n",
    "        if G.out_degree(start_node) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Find the longest path from this start node\n",
    "        path = find_longest_path_from_node(G, start_node)\n",
    "        \n",
    "        # Update longest path if this one is longer\n",
    "        if len(path) > len(longest_path):\n",
    "            longest_path = path\n",
    "    \n",
    "    return longest_path\n",
    "\n",
    "def find_longest_path_from_node(G, start_node):\n",
    "    \"\"\"\n",
    "    Find the longest path starting from a specific node.\n",
    "    \n",
    "    Args:\n",
    "        G: A NetworkX directed graph (DiGraph)\n",
    "        start_node: The starting node\n",
    "        \n",
    "    Returns:\n",
    "        A list of nodes representing the longest path from start_node\n",
    "    \"\"\"\n",
    "    # Use dynamic programming to find the longest path\n",
    "    # This is much more efficient than a brute force approach\n",
    "    \n",
    "    # Initialize distances and paths\n",
    "    dist = {node: -float('inf') for node in G.nodes()}\n",
    "    dist[start_node] = 0\n",
    "    pred = {node: None for node in G.nodes()}\n",
    "    \n",
    "    # Topologically sort the nodes\n",
    "    try:\n",
    "        topo_order = list(nx.topological_sort(G))\n",
    "    except nx.NetworkXUnfeasible:\n",
    "        # Graph has cycles, so we'll use a heuristic approach\n",
    "        # For simplicity, we'll use a DFS-based approach\n",
    "        visited = set()\n",
    "        path = [start_node]\n",
    "        current_path = []\n",
    "        dfs_longest_path(G, start_node, visited, path, current_path)\n",
    "        return current_path\n",
    "    \n",
    "    # Dynamic programming to find longest path\n",
    "    for node in topo_order:\n",
    "        for successor in G.successors(node):\n",
    "            if dist[successor] < dist[node] + 1:\n",
    "                dist[successor] = dist[node] + 1\n",
    "                pred[successor] = node\n",
    "    \n",
    "    # Find the node with the maximum distance\n",
    "    end_node = max(dist, key=dist.get)\n",
    "    \n",
    "    # Reconstruct the path\n",
    "    path = []\n",
    "    while end_node is not None:\n",
    "        path.append(end_node)\n",
    "        end_node = pred[end_node]\n",
    "    \n",
    "    # Reverse to get from start to end\n",
    "    return path[::-1]\n",
    "\n",
    "def dfs_longest_path(G, node, visited, path, longest_path):\n",
    "    \"\"\"\n",
    "    DFS helper for finding the longest path in a graph with cycles.\n",
    "    \n",
    "    Args:\n",
    "        G: A NetworkX directed graph (DiGraph)\n",
    "        node: Current node\n",
    "        visited: Set of visited nodes in current path\n",
    "        path: Current path\n",
    "        longest_path: Reference to the longest path found so far\n",
    "    \"\"\"\n",
    "    visited.add(node)\n",
    "    \n",
    "    for neighbor in G.successors(node):\n",
    "        if neighbor not in visited:\n",
    "            path.append(neighbor)\n",
    "            dfs_longest_path(G, neighbor, visited, path, longest_path)\n",
    "            path.pop()\n",
    "    \n",
    "    if len(path) > len(longest_path):\n",
    "        longest_path.clear()\n",
    "        longest_path.extend(path)\n",
    "    \n",
    "    visited.remove(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "76166615-b33c-4791-a089-a3fd3a673024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type_attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "7e5882f7-a249-49b5-8359-1014aaa25b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_graphml(mergaed_G_clean, wp+\"graphs/tem_history_graph_merge_clean.graphml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "7c032c2e-34f6-46a0-9723-adae7b8cf211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path 1: [syncope, lower extremity edema, sweating, short of breath, sharp, several years, Her shortness of breath, nausea, dyspnea, several years, Her lower extremity edema, a few seconds, nonradiating, chest twinges, cellulitis, vomiting] -- before --> the several weeks -- before --> [02/01/2002]\n",
      "Path 2: osteoarthritis -- before --> [walker, a cane] -- before --> [02/01/2002]\n",
      "Path 3: osteoarthritis -- before --> [leg pain, 2-3 years, increased shortness of breath, fevers, dyspnea, 2 1/2 years, an associated dry cough, orthopnea, noparoxysmal nocturnal dyspnea, chills, 5 days, Her shortness of breath] -- before --> [02/01/2002]\n",
      "Path 4: [headaches, hematuria, dyspnea, abdominal pain, melena, numbness, hematochezia, diarrhea, tingling, vision changes] -- before --> [02/01/2002]\n",
      "Path 5: [light headed] -- before --> [syncope, lower extremity edema, sweating, short of breath, sharp, several years, Her shortness of breath, nausea, dyspnea, several years, Her lower extremity edema, a few seconds, nonradiating, chest twinges, cellulitis, vomiting]\n",
      "Path 6: [hypertension, obesity] -- before --> [02/01/2002]\n",
      "Path 7: any pleural chest pain -- before --> [02/01/2002]\n"
     ]
    }
   ],
   "source": [
    "covering_paths = minimal_path_cover(mergaed_G_clean)\n",
    "G = mergaed_G_clean.copy()\n",
    "path_desc = []\n",
    "# print(\"Minimal path cover:\")\n",
    "for i, path in enumerate(covering_paths):\n",
    "    # print(f\"Path {i+1}: {path}\")\n",
    "    texts = []\n",
    "    for node in path:\n",
    "        \n",
    "        text = G.nodes[node].get(\"text\", None)\n",
    "        type = G.nodes[node].get(\"type\", None)\n",
    "        # time2section = G.nodes[node].get(\"time2section\", None)\n",
    "        if type == None:\n",
    "            tlist = []\n",
    "            type_attrs = G.nodes[node].get(\"node_attributes\")\n",
    "            type_attrs = json.loads(type_attrs)\n",
    "            for nid in type_attrs.keys():\n",
    "                # print(type_attrs[nid])\n",
    "                text = type_attrs[nid]['text']\n",
    "                type = type_attrs[nid]['type']\n",
    "                \n",
    "                # time2section = type_attrs[nid].get('time2section', None)\n",
    "                if type in ['PROBLEM', 'TEST', 'TREATMENT', 'DURATION', 'DATE', 'FREQUENCY']:\n",
    "                    tlist.append(text)\n",
    "                    # print(time2section, '....')\n",
    "            texts.append( '[' + ', '.join(tlist) + ']')\n",
    "        else:\n",
    "            if type in ['PROBLEM', 'TEST', 'TREATMENT', 'DURATION', 'DATE', 'FREQUENCY']:\n",
    "                texts.append(text)\n",
    "                # print(time2section)\n",
    "    # print(texts)\n",
    "    \n",
    "    path_desc.append(f\"Path {i+1}: \" + ' -- before --> '.join(texts)) \n",
    "print('\\n'.join(path_desc))\n",
    "min_paths = '\\n'.join(path_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201aab9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d4b36c-6b6a-454d-a386-6941abd4cc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO summarize path based on context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "1f36f08c-b34b-4547-adc2-2dea047c293a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from lmformatenforcer import JsonSchemaParser\n",
    "from pydantic import BaseModel\n",
    "import re, json, os\n",
    "from typing import Optional, Type, TypeVar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "ef6ba83d-2342-4812-89d1-2fb2fb44a6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify OpenAI's API key and API base to use vLLM's API server.\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://host.docker.internal:8000/v1\"\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "cf88c1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"Based on the provided text and the extracted temporal relation paths, \n",
    "construct a timeline for the events in the paths. \n",
    "The paths are given in the format where events are connected by temporal relations, \n",
    "such as A --before--> [B, C] --before--> D, where B and C happened at the same time. \n",
    "Each event or group of events should be placed on the timeline according to their temporal order.  \n",
    "Text:\n",
    "{history}\n",
    "\n",
    "Temporal Relation Paths:\n",
    "{min_paths}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "9e0c74fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided text and the extracted temporal relation paths, \n",
      "construct a timeline for the events in the paths. \n",
      "The paths are given in the format where events are connected by temporal relations, \n",
      "such as A --before--> [B, C] --before--> D, where B and C happened at the same time. \n",
      "Each event or group of events should be placed on the timeline according to their temporal order.  \n",
      "Text:\n",
      "HISTORY OF PRESENT ILLNESS :\n",
      "Saujule Study is a 77-year-old woman with a history of obesity and hypertension who presents with increased shortness of breath x 5 days .\n",
      "Her shortness of breath has been progressive over the last 2-3 years .\n",
      "She has an associated dry cough but no fevers , chills , or leg pain .\n",
      "She has dyspnea on exertion .\n",
      "She ambulates with walker and a cane secondary to osteoarthritis .\n",
      "She becomes short of breath just by getting up from her chair and can only walk 2-3 steps on a flat surface .\n",
      "She feels light headed when getting up .\n",
      "Her shortness of breath and dyspnea on exertion has been progressive for the past several years .\n",
      "It has not been sudden or acute .\n",
      "She sleeps in a chair up right for the last 2 1/2 years secondary to osteoarthritis .\n",
      "She has orthopnea as well but noparoxysmal nocturnal dyspnea .\n",
      "She occasionally feels chest twinges which are nonradiating but are sharp .\n",
      "They last a few seconds on the left side and are not associated with sweating , nausea , vomiting or syncope .\n",
      "She has had lower extremity edema for thelast several years with multiple episodes of cellulitis .\n",
      "Her lower extremity edema has increased for the several weeks prior to admission secondary to an inability to elevate her legs due to a broken chair at home .\n",
      "She denies any pleural chest pain .\n",
      "REVIEW OF SYSTEMS :\n",
      "Negative for headaches , vision changes , numbness , tingling , dyspnea , hematuria , abdominal pain , diarrhea , melena or hematochezia .\n",
      "\n",
      "\n",
      "Temporal Relation Paths:\n",
      "Path 1: [syncope, lower extremity edema, sweating, short of breath, sharp, several years, Her shortness of breath, nausea, dyspnea, several years, Her lower extremity edema, a few seconds, nonradiating, chest twinges, cellulitis, vomiting] -- before --> the several weeks -- before --> [02/01/2002]\n",
      "Path 2: osteoarthritis -- before --> [walker, a cane] -- before --> [02/01/2002]\n",
      "Path 3: osteoarthritis -- before --> [leg pain, 2-3 years, increased shortness of breath, fevers, dyspnea, 2 1/2 years, an associated dry cough, orthopnea, noparoxysmal nocturnal dyspnea, chills, 5 days, Her shortness of breath] -- before --> [02/01/2002]\n",
      "Path 4: [headaches, hematuria, dyspnea, abdominal pain, melena, numbness, hematochezia, diarrhea, tingling, vision changes] -- before --> [02/01/2002]\n",
      "Path 5: [light headed] -- before --> [syncope, lower extremity edema, sweating, short of breath, sharp, several years, Her shortness of breath, nausea, dyspnea, several years, Her lower extremity edema, a few seconds, nonradiating, chest twinges, cellulitis, vomiting]\n",
      "Path 6: [hypertension, obesity] -- before --> [02/01/2002]\n",
      "Path 7: any pleural chest pain -- before --> [02/01/2002]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "ea3e6dec-19b5-45f4-94d9-0128065d4fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat response: Okay, I need to create a timeline based on the provided text and the extracted temporal relation paths. Let me start by understanding the events and how they connect.\n",
      "\n",
      "First, I see multiple paths with events connected by \"before.\" Each path has a sequence of events or groups that occur before a specific date, 02/01/2002. Some events happen several years before, others are more recent, like 5 days before.\n",
      "\n",
      "Looking at Path 1: It mentions syncope, lower extremity edema, and other symptoms that occurred several years before, then moving to several weeks prior to 02/01/2002.\n",
      "\n",
      "Path 2 connects osteoarthritis to using a walker and cane, which happened before 02/01/2002.\n",
      "\n",
      "Path 3 ties osteoarthritis to several conditions like leg pain, increased shortness of breath, dry cough, etc., all before 2002.\n",
      "\n",
      "Path 4 lists negative symptoms that are before 2002.\n",
      "\n",
      "Path 5 links feeling light-headed to other symptoms before 2002.\n",
      "\n",
      "Path 6 connects hypertension and obesity before 2002.\n",
      "\n",
      "Path 7 mentions absence of pleural chest pain before 2002.\n",
      "\n",
      "I need to organize all these events in a coherent timeline, grouping similar events together, and noting the temporal relations correctly. The main time markers are several years, several weeks, and 5 days before the date.\n",
      "\n",
      "I'll start from the earliest events, like the long-term conditions, then go to the more recent ones, like the broken chair leading to increased edema several weeks before admission.\n",
      "\n",
      "I should list each event or group, specify when they occurred relative to 02/01/2002, and ensure the timeline flows logically, showing the progression of the patient's condition over time.\n",
      "</think>\n",
      "\n",
      "### Timeline of Events Based on Temporal Relation Paths\n",
      "\n",
      "1. **Several Years Before 02/01/2002:**\n",
      "   - **Osteoarthritis:** Developed.\n",
      "   - **Increased Shortness of Breath:** Begins and progressively worsens over 2-3 years.\n",
      "   - **Dry Cough:** Develops.\n",
      "   - **Lower Extremity Edema:** Develops and increases over several years with episodes of cellulitis.\n",
      "   - **Dyspnea on Exertion:** Develops and progressively worsens for several years.\n",
      "   - **Orthopnea:** Develops.\n",
      "   - **Chest Twinges:** Occasional, nonradiating, sharp twinges lasting a few seconds on the left side.\n",
      "\n",
      "2. **Several Weeks Before 02/01/2002:**\n",
      "   - **Lower Extremity Edema:** Increases due to inability to elevate legs because of a broken chair at home.\n",
      "\n",
      "3. **5 Days Before 02/01/2002:**\n",
      "   - **Increased Shortness of Breath:** Becomes more pronounced.\n",
      "\n",
      "4. **Events Before 02/01/2002 (No Specific Timing):**\n",
      "   - **Hypertension and Obesity:** Pre-existing conditions.\n",
      "   - **Osteoarthritis:** Leads to use of walker and cane.\n",
      "   - **Light-Headedness:** Occurs when getting up.\n",
      "   - **Sleeping in Upright Chair:** For the last 2.5 years due to osteoarthritis.\n",
      "   - **Negative for Pleural Chest Pain, Headaches, Vision Changes, Numbness, Tingling, Dyspnea, Hematuria, Abdominal Pain, Diarrhea, Melena, Hematochezia:** No reported issues.\n",
      "\n",
      "---\n",
      "\n",
      "### Timeline Summary:\n",
      "\n",
      "- **Several Years Before 02/01/2002:** Onset and progression of osteoarthritis, increased shortness of breath, dry cough, lower extremity edema, dyspnea on exertion, and chest twinges.\n",
      "- **Several Weeks Before 02/01/2002:** Increased lower extremity edema due to inability to elevate legs.\n",
      "- **5 Days Before 02/01/2002:** Increased shortness of breath becomes more pronounced.\n",
      "- **Before 02/01/2002 (No Specific Timing):** Use of walker and cane due to osteoarthritis, sleeping upright in a chair, and absence of certain symptoms (e.g., pleural chest pain, headaches, etc.).\n",
      "\n",
      "This timeline captures the progression of symptoms and conditions leading up to 02/01/2002.\n"
     ]
    }
   ],
   "source": [
    "chat_response = client.chat.completions.create(\n",
    "    model = 'deepseek-ai/DeepSeek-R1-Distill-Qwen-14B',\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    ")\n",
    "print(\"Chat response:\", chat_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "7c860b5b-0d67-4aa7-8874-ad425f49522c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat response: To find the date that is 5 days before February 1, 2002, I need to count backward from that date.\n",
      "\n",
      "First, I'll subtract 5 days from February 1, 2002. Since January has 31 days, 5 days before February 1 would land in January.\n",
      "\n",
      "Specifically, January has 31 days, so subtracting 5 days from February 1 (which is the 32nd day of January) would result in January 26.\n",
      "\n",
      "Therefore, the date 5 days before February 1, 2002 is January 26, 2002.\n",
      "</think>\n",
      "\n",
      "To find the date that is **5 Days Before** February 1, 2002, follow these easy steps:\n",
      "\n",
      "1. **Identify the Target Date:**\n",
      "   \n",
      "   We are looking for the date that is **5 days before** February 1, 2002.\n",
      "\n",
      "2. **Subtract the Days:**\n",
      "   \n",
      "   Subtract 5 days from February 1, 2002.\n",
      "\n",
      "3. **Calculate the Date:**\n",
      "   \n",
      "   - February 1, 2002 minus 5 days lands us in **January 2002**.\n",
      "   - Specifically, January has **31 days**, so counting back:\n",
      "     \n",
      "     - **February 1, 2002** is the 32nd day of January (since January has 31 days).\n",
      "     - Subtracting 5 days from the 32nd day of January brings us to **January 26, 2002**.\n",
      "\n",
      "\\[\n",
      "\\boxed{\\text{January } 26,\\ 2002}\n",
      "\\]\n"
     ]
    }
   ],
   "source": [
    "chat_response = client.chat.completions.create(\n",
    "    model = 'deepseek-ai/DeepSeek-R1-Distill-Qwen-14B',\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": 'what is the date of 5 Days Before 02/01/2002'},\n",
    "    ]\n",
    ")\n",
    "print(\"Chat response:\", chat_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293a0abf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuhk",
   "language": "python",
   "name": "cuhk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
