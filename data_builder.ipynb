{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c28e864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda env list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "970f6ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install xmltodict\n",
    "# %pip install isodate\n",
    "# %pip install pandas\n",
    "# %pip install --no-cache-dir --force-reinstall numpy\n",
    "# %pip install --upgrade scipy pandas\n",
    "# %pip install lm-format-enforcer\n",
    "# %pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b089890b-8dc0-4333-8b76-fcce824317fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xmltodict, json\n",
    "import os\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from itertools import combinations\n",
    "import json, re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ea9dd3-6fc4-42ac-96b0-af9954baab60",
   "metadata": {},
   "source": [
    "### Load raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0ec8452a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pwd\n",
    "# !conda env list\n",
    "# !python --version\n",
    "# !cd /home/jovyan/work/Temporal_relation/\n",
    "# !pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ceeee538-9c9a-4043-8750-f04b4a692afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to path to the data\n",
    "# path = '/home/wt/Downloads/n2c2 2012/'\n",
    "wp = '/home/jovyan/work/Temporal_relation/'\n",
    "path = wp + 'data/i2b2/'\n",
    "training_data_path = path + 'merge_training'\n",
    "test_data_path = path + 'ground_truth/merged_xml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "03b04a12-9b35-446c-8984-8dffb38b6575",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(data_path):\n",
    "    data = {}\n",
    "    for filename in os.listdir(data_path):\n",
    "        if filename.endswith(\".xml\"): \n",
    "            f = (os.path.join(data_path, filename))\n",
    "#             print(f)\n",
    "            fb = open(f, \"rb\").read().decode(encoding=\"utf-8\")\n",
    "#     invalid character '&' https://github.com/martinblech/xmltodict/issues/277\n",
    "            fb = fb.replace('&', '&amp;')\n",
    "            dic = xmltodict.parse(fb, attr_prefix='')\n",
    "#     restore orginal character \"&\"\n",
    "            dic['ClinicalNarrativeTemporalAnnotation']['TEXT'] = dic['ClinicalNarrativeTemporalAnnotation']['TEXT'].replace('&amp;', '&')\n",
    "            data[filename] = (dic)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "58df447f-02f8-4e82-a7cc-e48cf806524e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data_loader(training_data_path)\n",
    "test_data = data_loader(test_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f519491c-a84d-4f5f-a9f9-1c2fcbd18ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190 120\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data), len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6e3bdd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dateutil.relativedelta import relativedelta\n",
    "# import isodate\n",
    "# from datetime import datetime\n",
    "\n",
    "# anchor_date = datetime(2002, 2, 1)  # example from TIMEX3 id T0\n",
    "\n",
    "# # Example: Convert \"P3Y\" to a relativedelta\n",
    "# duration_str = \"P3Y\"\n",
    "# delta = isodate.parse_duration(duration_str)  # delta will be a timedelta or duration object\n",
    "# # For years/months, use: \n",
    "# rd = relativedelta(years=3)  # if P3Y specifically means 3 years\n",
    "\n",
    "# # Now, combine with the anchor date:\n",
    "# resolved_date = anchor_date + rd\n",
    "# print(resolved_date)  # 2005-02-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529f0de8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f6fdb414-c896-447c-927b-8cb13664e9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_first_regex(text, substrings):\n",
    "    pattern = '|'.join(map(re.escape, substrings))  # Escape special characters\n",
    "    match = re.search(pattern, text)\n",
    "    if match:\n",
    "        return match.start()\n",
    "    else:\n",
    "        raise ValueError(\"None of the substrings found in the text.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ecbb04f1-43c5-44d6-80c1-8ee4ef63bdde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   text polarity\n",
      "0           happy event      POS\n",
      "1     no bad experience      NEG\n",
      "2         joyful moment      POS\n",
      "3  no terrible incident      NEG\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    'text': ['happy event', 'bad experience', 'joyful moment', 'terrible incident'],\n",
    "    'polarity': ['POS', 'NEG', 'POS', 'NEG']\n",
    "}\n",
    "events = pd.DataFrame(data)\n",
    "\n",
    "# Prepend \"no \" to the \"text\" column for rows where polarity is 'NEG'\n",
    "events.loc[events['polarity'] == 'NEG', 'text'] = 'no ' + events['text']\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c5928d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_section_graph(doc_id, data, section='all'):\n",
    "    # for doc_id in list(data.keys())[:1]:\n",
    "    text = data[doc_id]['ClinicalNarrativeTemporalAnnotation']['TEXT']\n",
    "    # print(text)\n",
    "    \n",
    "    history_start = text.index('HISTORY OF PRESENT ILLNESS ')\n",
    "    substrings = ['HOSPITAL COURSE']\n",
    "    history_end = find_first_regex(text, substrings)\n",
    "\n",
    "    sect_start, sect_end = 0, len(text)\n",
    "    if section == 'history':\n",
    "        sect_start, sect_end = history_start, history_end\n",
    "    elif section == 'other':\n",
    "        sect_start, sect_end = history_end, len(text)\n",
    "    # print(text[sect_start:sect_end])\n",
    "\n",
    "    events = pd.DataFrame(data[doc_id]['ClinicalNarrativeTemporalAnnotation']['TAGS']['EVENT'])\n",
    "    events['start'] = events['start'].astype(int)\n",
    "    events['end'] = events['end'].astype(int)\n",
    "    # Filter events in the history section\n",
    "    # print('events', events.shape)\n",
    "    events = events.loc[(events['start']>=sect_start) & (events['end']<=sect_end)]\n",
    "    # print('events after', events.shape)\n",
    "    events.loc[events['polarity'] == 'NEG', 'text'] = 'no ' + events['text']\n",
    "    \n",
    "    # FILTER 1: only use events related to medical concepts\n",
    "    # events = events.loc[events['type'].isin(['PROBLEM', 'TEST', 'TREATMENT'])]\n",
    "    event_types = dict(zip(events['id'], events['type']))\n",
    "    event_values = dict(zip(events['id'], events['modality']))\n",
    "    \n",
    "    # Remove duplicated admission and discharge time.\n",
    "    # adm_dis = pd.DataFrame(data[doc_id]['ClinicalNarrativeTemporalAnnotation']['TAGS']['SECTIME'])\n",
    "    times = pd.DataFrame(data[doc_id]['ClinicalNarrativeTemporalAnnotation']['TAGS']['TIMEX3'])\n",
    "    times['start'] = times['start'].astype(int)\n",
    "    times['end'] = times['end'].astype(int)\n",
    "    # print('times', times.shape)\n",
    "    if section == 'history':\n",
    "        times = times.loc[((times['start']>=sect_start) & (times['end']<=sect_end)) | (times['id']=='T0')]\n",
    "    elif section == 'other':\n",
    "        times = times.loc[((times['start']>=sect_start) & (times['end']<=sect_end))| (times['id']=='T1')]\n",
    "    else:\n",
    "        times = times.loc[((times['start']>=sect_start) & (times['end']<=sect_end))| (times['id'].isin(['T0', 'T1']))]\n",
    "    # print('times after', times.shape)\n",
    "    time_types = dict(zip(times['id'], times['type']))\n",
    "    time_values = dict(zip(times['id'], times['val']))\n",
    "\n",
    "    \n",
    "    nodes_keep = list(events['id']) + list(times['id'])\n",
    "    # print(len(nodes_keep))\n",
    "    \n",
    "    all_links = pd.DataFrame(data[doc_id]['ClinicalNarrativeTemporalAnnotation']['TAGS']['TLINK'])\n",
    "    all_links = all_links.loc[all_links['type']!='']\n",
    "\n",
    "    # SELECTION 1: only use events related to time expressions\n",
    "    # Select key-event based on dates are not enough, as there are many events that are not related to time expressions. \n",
    "    # tlinks = all_links.loc[(all_links['fromID'].isin(time_types)) | (all_links['toID'].isin(time_types))]\n",
    "    # tlinks = tlinks.loc[(tlinks['id'].str.lower().str.contains('sectime')==False)]\n",
    "    # key_events = set(tlinks.loc[(tlinks['fromID'].isin(event_types))]['fromID'].unique()) | set(tlinks.loc[(tlinks['toID'].isin(event_types))]['toID'].unique())\n",
    "    # print(len(key_events))\n",
    "\n",
    "    \n",
    "    # FILTER 2: Exclude sectime links not about admission/disharge\n",
    "    if section == 'history':\n",
    "        links = all_links.loc[(all_links['id'].str.lower().str.contains('sectime')==False) | ((all_links['id'].str.lower().str.contains('sectime')==True) & (all_links['toID']=='T0'))]\n",
    "    elif section == 'other':\n",
    "        links = all_links.loc[(all_links['id'].str.lower().str.contains('sectime')==False) | ((all_links['id'].str.lower().str.contains('sectime')==True) & (all_links['toID']=='T1'))]\n",
    "    else:\n",
    "        links = all_links\n",
    "    \n",
    "    \n",
    "    # Normalize AFTER and BEFORE relations\n",
    "    links = links.copy()\n",
    "    mask = (links['type'] == 'AFTER')\n",
    "    links.loc[mask, ['fromID', 'fromText', 'toID', 'toText']] = links.loc[mask, ['toID', 'toText', 'fromID', 'fromText']].values\n",
    "    links.loc[mask, 'type'] = 'BEFORE'\n",
    "    links = links.drop_duplicates(subset=['fromID', 'fromText', 'toID', 'toText', 'type'], keep='last')\n",
    "    \n",
    "    \n",
    "    G = nx.from_pandas_edgelist(links[['fromID', 'toID', 'type']], source='fromID', target='toID', edge_attr=True, create_using=nx.DiGraph())\n",
    "    # source_nodes = dict(zip(links['fromID'], links['fromText']))\n",
    "    # target_nodes = dict(zip(links['toID'], links['toText']))\n",
    "    # nx.set_node_attributes(G, source_nodes|target_nodes, 'text')\n",
    "    nx.set_node_attributes(G, dict(zip(events['id'], events['text']))|dict(zip(times['id'], times['text'])), 'text')\n",
    "    nx.set_node_attributes(G, event_types|time_types, 'type')\n",
    "    nx.set_node_attributes(G, event_values|time_values, 'norm_time')\n",
    "    \n",
    "    # only keep nodes of interest\n",
    "    # FILTER 3: only subgraph\n",
    "    G = G.subgraph(nodes_keep).copy()\n",
    "    \n",
    "    # clear reverse links and reduce redundent nodes; \n",
    "    # There are no many duplicated links\n",
    "    return G, text[sect_start:sect_end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a17798a3-14b1-42ac-bb25-b2261929447a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All nodes and links in the data;\n",
    "# G, text = build_section_graph('36.xml', train_data, 'all')\n",
    "# Only nodes and links in the history section\n",
    "G, history = build_section_graph('2.xml', train_data, 'all')\n",
    "# Only nodes and links in sections other than \"history\" section\n",
    "# G, text = build_section_graph('36.xml', train_data, 'other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7e9a079e-801b-45a8-aab9-736669887c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph with 177 nodes and 331 edges\n"
     ]
    }
   ],
   "source": [
    "print(f\"Graph with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1c4ea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len([path in nx.all_simple_paths(G, source='T2', target='E87')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "571334a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of weakly connected components: 1\n",
      "Component sizes: [56]\n"
     ]
    }
   ],
   "source": [
    "num_components = nx.number_weakly_connected_components(G)\n",
    "print(f\"Number of weakly connected components: {num_components}\")\n",
    "\n",
    "weakly_connected = list(nx.weakly_connected_components(G))\n",
    "print(f\"Component sizes: {[len(comp) for comp in weakly_connected]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "7bac7149-1cac-40e6-b5ed-595b1faa4c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nx.write_graphml(G, wp+\"graphs/tem_history_graph.graphml\")\n",
    "# nx.write_graphml(G, wp+\"graphs/tem_hospital_graph.graphml\")\n",
    "# nx.write_graphml(G, wp+\"graphs/tem_graph_key_events_remove_sect.graphml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3b70aa-49ae-4894-b96d-3947694c9f8d",
   "metadata": {},
   "source": [
    "### Merge overlap nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "4c5c0909-72d8-496a-8a9b-5a0b5597aa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_overlapping_nodes(G):\n",
    "    \"\"\"\n",
    "    Merge nodes that are connected by edges with type='overlp'.\n",
    "    Only merges edges across merged groups if they have the same direction.\n",
    "    Maintains node attributes and edge attributes as JSON strings for GraphML compatibility.\n",
    "    Tracks source nodes for each merged edge.\n",
    "    \n",
    "    Args:\n",
    "        G (nx.Graph or nx.DiGraph): Input graph\n",
    "        \n",
    "    Returns:\n",
    "        nx.Graph or nx.DiGraph: New graph with merged nodes and edges\n",
    "    \"\"\"\n",
    "    # Create new merged graph of same type as input\n",
    "    merged_G = G.__class__()\n",
    "    \n",
    "    # Find connected components considering only overlap edges\n",
    "    overlap_edges = [(u, v) for u, v, d in G.edges(data=True) if d.get('type') == 'OVERLAP']\n",
    "    overlap_graph = nx.Graph()  # Undirected for finding components\n",
    "    overlap_graph.add_edges_from(overlap_edges)\n",
    "    \n",
    "    # Get clusters of nodes to merge\n",
    "    clusters = list(nx.connected_components(overlap_graph))\n",
    "    \n",
    "    # Create mapping from original nodes to their merged cluster names\n",
    "    node_to_cluster = {}\n",
    "    for cluster in clusters:\n",
    "        cluster = list(cluster)\n",
    "        merged_name = '+'.join(sorted(cluster))\n",
    "        for node in cluster:\n",
    "            node_to_cluster[node] = merged_name\n",
    "    \n",
    "    # Process nodes\n",
    "    for cluster in clusters:\n",
    "        cluster = list(cluster)\n",
    "        \n",
    "        if len(cluster) == 1:\n",
    "            # Single node, just copy it and its attributes\n",
    "            node = cluster[0]\n",
    "            merged_G.add_node(node, **G.nodes[node])\n",
    "            continue\n",
    "            \n",
    "        # Create merged node name\n",
    "        merged_node = '+'.join(sorted(cluster))\n",
    "        \n",
    "        # Combine node attributes and convert to JSON string\n",
    "        merged_attrs = {\n",
    "            'original_nodes': json.dumps(cluster),\n",
    "            'node_attributes': json.dumps({node: dict(G.nodes[node]) for node in cluster})\n",
    "        }\n",
    "        \n",
    "        # Add merged node\n",
    "        merged_G.add_node(merged_node, **merged_attrs)\n",
    "    \n",
    "    # Add nodes that weren't in any cluster\n",
    "    unclustered_nodes = set(G.nodes()) - set(node for cluster in clusters for node in cluster)\n",
    "    for node in unclustered_nodes:\n",
    "        merged_G.add_node(node, **G.nodes[node])\n",
    "        node_to_cluster[node] = node  # Map to itself\n",
    "    \n",
    "    # Create a dictionary to store edges between clusters\n",
    "    cluster_edges = {}  # (from_cluster, to_cluster) -> list of original edges with source info\n",
    "    \n",
    "    # Process edges\n",
    "    for u, v, data in G.edges(data=True):\n",
    "        # Get cluster names (or original names for unclustered nodes)\n",
    "        u_cluster = node_to_cluster[u]\n",
    "        v_cluster = node_to_cluster[v]\n",
    "        \n",
    "        # Skip internal edges of merged clusters if they were overlap edges\n",
    "        if u_cluster == v_cluster and data.get('type') == 'OVERLAP':\n",
    "            continue\n",
    "        \n",
    "        # Add source node information to edge data\n",
    "        edge_data = data.copy()\n",
    "        edge_data['source_nodes'] = {'from': u, 'to': v}\n",
    "        \n",
    "        # Create edge key based on direction\n",
    "        edge_key = (u_cluster, v_cluster)\n",
    "        \n",
    "        # For directed graphs, maintain direction information\n",
    "        if isinstance(G, nx.DiGraph):\n",
    "            if edge_key not in cluster_edges:\n",
    "                cluster_edges[edge_key] = []\n",
    "            cluster_edges[edge_key].append((u, v, edge_data))\n",
    "        else:\n",
    "            # For undirected graphs, normalize the edge key\n",
    "            normalized_key = tuple(sorted([u_cluster, v_cluster]))\n",
    "            if normalized_key not in cluster_edges:\n",
    "                cluster_edges[normalized_key] = []\n",
    "            cluster_edges[normalized_key].append((u, v, edge_data))\n",
    "    \n",
    "    # Add merged edges to the graph\n",
    "    for (from_cluster, to_cluster), edges in cluster_edges.items():\n",
    "        # For directed graphs, check if all edges have the same direction\n",
    "        if isinstance(G, nx.DiGraph):\n",
    "            # Check if all edges go in the same direction\n",
    "            directions = set((u_cluster, v_cluster) \n",
    "                           for u, v, _ in edges\n",
    "                           for u_cluster, v_cluster in [(node_to_cluster[u], node_to_cluster[v])])\n",
    "            \n",
    "            if len(directions) == 1:  # All edges have same direction\n",
    "                merged_G.add_edge(from_cluster, to_cluster, \n",
    "                                edge_attributes=json.dumps([data for _, _, data in edges]))\n",
    "        else:\n",
    "            # For undirected graphs, just add the edge\n",
    "            merged_G.add_edge(from_cluster, to_cluster, \n",
    "                            edge_attributes=json.dumps([data for _, _, data in edges]))\n",
    "    \n",
    "    return merged_G\n",
    "\n",
    "def load_merged_graph(graphml_file):\n",
    "    \"\"\"\n",
    "    Load a merged graph from GraphML file and convert JSON string attributes back to Python objects.\n",
    "    \"\"\"\n",
    "    G = nx.read_graphml(graphml_file)\n",
    "    \n",
    "    # Convert node attributes back from JSON\n",
    "    for node in G.nodes():\n",
    "        if 'original_nodes' in G.nodes[node]:\n",
    "            G.nodes[node]['original_nodes'] = json.loads(G.nodes[node]['original_nodes'])\n",
    "            G.nodes[node]['node_attributes'] = json.loads(G.nodes[node]['node_attributes'])\n",
    "    \n",
    "    # Convert edge attributes back from JSON\n",
    "    for u, v in G.edges():\n",
    "        if 'edge_attributes' in G[u][v]:\n",
    "            G[u][v]['edge_attributes'] = json.loads(G[u][v]['edge_attributes'])\n",
    "    \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "68014572-7ea3-48de-9a7a-bec02d2c9cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mergaed_G = merge_overlapping_nodes(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "4e20d87b-48e0-49bf-ab5f-b7d66b66d521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph with 14 nodes and 20 edges\n"
     ]
    }
   ],
   "source": [
    "print(f\"Graph with {mergaed_G.number_of_nodes()} nodes and {mergaed_G.number_of_edges()} edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "97c310c4-9f7e-450f-b891-66fb2303fc9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T8 {'text': 'the several weeks', 'type': 'DURATION', 'norm_time': 'P3W'}\n"
     ]
    }
   ],
   "source": [
    "node = 'T8'\n",
    "print(node, mergaed_G.nodes[node])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "ceb67fa0-b009-4592-92d8-d5f927263339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nx.write_graphml(mergaed_G, wp+\"graphs/tem_history_graph_merge.graphml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "0a9f53a9-a5cb-4643-9257-3de36fc85d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(list(mergaed_G.nodes(data=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02b554d-ba69-413d-a6cf-9755aaa5c841",
   "metadata": {},
   "source": [
    "### Remove conflicting relations (e.g., self-link and mutual links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "a799f672-1938-4398-9e6f-017630da4983",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_self_links(G):\n",
    "    H = G.copy()\n",
    "    self_loops = list(nx.selfloop_edges(H))\n",
    "    H.remove_edges_from(self_loops)\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "37831800-2134-45b6-8aac-5b67b35d2f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_mutual_links(G):\n",
    "    H = G.copy()\n",
    "    edges_to_remove = set()\n",
    "    \n",
    "    for u, v in G.edges():\n",
    "        if H.has_edge(v, u) and (v, u) not in edges_to_remove and (u, v) not in edges_to_remove:\n",
    "            edge1 = G.get_edge_data(u, v)\n",
    "            edge2 = G.get_edge_data(v, u)\n",
    "            edges_to_remove.add((u, v))\n",
    "            edges_to_remove.add((v, u))\n",
    "    H.remove_edges_from(edges_to_remove)\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "c393bd1b-f14b-476b-a1d2-1a37f6525f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mergaed_G_clean = remove_self_links(mergaed_G)\n",
    "mergaed_G_clean = remove_mutual_links(mergaed_G_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "59f526db-d5a7-4d8c-b172-ed057a9ea8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph with 14 nodes and 20 edges\n"
     ]
    }
   ],
   "source": [
    "print(f\"Graph with {mergaed_G_clean.number_of_nodes()} nodes and {mergaed_G_clean.number_of_edges()} edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "f905a946-cf4b-4c21-9dce-2bd51d863d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T8 {'text': 'the several weeks', 'type': 'DURATION', 'norm_time': 'P3W'}\n"
     ]
    }
   ],
   "source": [
    "node = 'T8'\n",
    "print(node, mergaed_G_clean.nodes[node])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f653d8-94fc-44b6-83ea-07b65e5b951e",
   "metadata": {},
   "source": [
    "### Remove redundant links (shortcuts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "c1384a7b-ae17-4dd6-97aa-9a0ef1eebb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_redundant_edges(G):\n",
    "    redundant_edges = []\n",
    "    \n",
    "    # Iterate over edges while capturing their attributes\n",
    "    edges = list(G.edges(data=True))  # List of tuples (u, v, data_dict)\n",
    "    \n",
    "    for u, v, data in edges:\n",
    "        # Remove the edge and check if a path still exists\n",
    "        G.remove_edge(u, v)\n",
    "        \n",
    "        if nx.has_path(G, u, v):\n",
    "            redundant_edges.append((u, v))\n",
    "        \n",
    "        # Re-add the edge with its original attributes\n",
    "        G.add_edge(u, v, **data)\n",
    "    \n",
    "    # Remove redundant edges (preserves attributes of non-redundant edges)\n",
    "    G.remove_edges_from(redundant_edges)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "d6519493-50cd-4d39-9b13-a880c9c53ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mergaed_G_clean = remove_redundant_edges(mergaed_G_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "7b14055e-15d8-4219-82ba-630f3e8e691c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T8 {'text': 'the several weeks', 'type': 'DURATION', 'norm_time': 'P3W'}\n"
     ]
    }
   ],
   "source": [
    "node = 'T8'\n",
    "print(node, mergaed_G_clean.nodes[node])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "9806404d-6529-4659-851d-ce5f1dea5e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph with 14 nodes and 13 edges\n"
     ]
    }
   ],
   "source": [
    "print(f\"Graph with {mergaed_G_clean.number_of_nodes()} nodes and {mergaed_G_clean.number_of_edges()} edges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd70f989-585a-47cd-b7df-2714eb22844d",
   "metadata": {},
   "source": [
    "### Minimal paths and clean nodes rather than 'PROBLEM', 'TEST', 'TREATMENT'. A --> B --> C  and D-->C "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "ae72d5cf-46bf-4953-bc20-fbe015a5276e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimal_path_cover(G):\n",
    "    \"\"\"\n",
    "    Find a minimal collection of paths that cover all edges in a directed graph.\n",
    "    \n",
    "    Args:\n",
    "        G: A NetworkX directed graph (DiGraph)\n",
    "        \n",
    "    Returns:\n",
    "        A list of paths, where each path is a list of nodes\n",
    "    \"\"\"\n",
    "    if not G.edges():\n",
    "        return []\n",
    "    \n",
    "    # Create a working copy of the graph\n",
    "    remaining_edges = G.copy()\n",
    "    paths = []\n",
    "    \n",
    "    while remaining_edges.edges():\n",
    "        # Find longest path in the remaining graph\n",
    "        # This is a greedy approach - finding the truly minimal cover is NP-hard\n",
    "        longest_path = find_longest_path(remaining_edges)\n",
    "        \n",
    "        # Add the path to our collection\n",
    "        paths.append(longest_path)\n",
    "        \n",
    "        # Remove the edges in this path from the remaining graph\n",
    "        for i in range(len(longest_path) - 1):\n",
    "            u, v = longest_path[i], longest_path[i + 1]\n",
    "            if remaining_edges.has_edge(u, v):\n",
    "                remaining_edges.remove_edge(u, v)\n",
    "    \n",
    "    return paths\n",
    "\n",
    "def find_longest_path(G):\n",
    "    \"\"\"\n",
    "    Find the longest path in a directed graph.\n",
    "    \n",
    "    Args:\n",
    "        G: A NetworkX directed graph (DiGraph)\n",
    "        \n",
    "    Returns:\n",
    "        A list of nodes representing the longest path\n",
    "    \"\"\"\n",
    "    # For each node, try to find the longest path starting from it\n",
    "    longest_path = []\n",
    "    \n",
    "    for start_node in G.nodes():\n",
    "        # Skip nodes with no outgoing edges\n",
    "        if G.out_degree(start_node) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Find the longest path from this start node\n",
    "        path = find_longest_path_from_node(G, start_node)\n",
    "        \n",
    "        # Update longest path if this one is longer\n",
    "        if len(path) > len(longest_path):\n",
    "            longest_path = path\n",
    "    \n",
    "    return longest_path\n",
    "\n",
    "def find_longest_path_from_node(G, start_node):\n",
    "    \"\"\"\n",
    "    Find the longest path starting from a specific node.\n",
    "    \n",
    "    Args:\n",
    "        G: A NetworkX directed graph (DiGraph)\n",
    "        start_node: The starting node\n",
    "        \n",
    "    Returns:\n",
    "        A list of nodes representing the longest path from start_node\n",
    "    \"\"\"\n",
    "    # Use dynamic programming to find the longest path\n",
    "    # This is much more efficient than a brute force approach\n",
    "    \n",
    "    # Initialize distances and paths\n",
    "    dist = {node: -float('inf') for node in G.nodes()}\n",
    "    dist[start_node] = 0\n",
    "    pred = {node: None for node in G.nodes()}\n",
    "    \n",
    "    # Topologically sort the nodes\n",
    "    try:\n",
    "        topo_order = list(nx.topological_sort(G))\n",
    "    except nx.NetworkXUnfeasible:\n",
    "        # Graph has cycles, so we'll use a heuristic approach\n",
    "        # For simplicity, we'll use a DFS-based approach\n",
    "        visited = set()\n",
    "        path = [start_node]\n",
    "        current_path = []\n",
    "        dfs_longest_path(G, start_node, visited, path, current_path)\n",
    "        return current_path\n",
    "    \n",
    "    # Dynamic programming to find longest path\n",
    "    for node in topo_order:\n",
    "        for successor in G.successors(node):\n",
    "            if dist[successor] < dist[node] + 1:\n",
    "                dist[successor] = dist[node] + 1\n",
    "                pred[successor] = node\n",
    "    \n",
    "    # Find the node with the maximum distance\n",
    "    end_node = max(dist, key=dist.get)\n",
    "    \n",
    "    # Reconstruct the path\n",
    "    path = []\n",
    "    while end_node is not None:\n",
    "        path.append(end_node)\n",
    "        end_node = pred[end_node]\n",
    "    \n",
    "    # Reverse to get from start to end\n",
    "    return path[::-1]\n",
    "\n",
    "def dfs_longest_path(G, node, visited, path, longest_path):\n",
    "    \"\"\"\n",
    "    DFS helper for finding the longest path in a graph with cycles.\n",
    "    \n",
    "    Args:\n",
    "        G: A NetworkX directed graph (DiGraph)\n",
    "        node: Current node\n",
    "        visited: Set of visited nodes in current path\n",
    "        path: Current path\n",
    "        longest_path: Reference to the longest path found so far\n",
    "    \"\"\"\n",
    "    visited.add(node)\n",
    "    \n",
    "    for neighbor in G.successors(node):\n",
    "        if neighbor not in visited:\n",
    "            path.append(neighbor)\n",
    "            dfs_longest_path(G, neighbor, visited, path, longest_path)\n",
    "            path.pop()\n",
    "    \n",
    "    if len(path) > len(longest_path):\n",
    "        longest_path.clear()\n",
    "        longest_path.extend(path)\n",
    "    \n",
    "    visited.remove(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "76166615-b33c-4791-a089-a3fd3a673024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type_attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "7e5882f7-a249-49b5-8359-1014aaa25b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nx.write_graphml(mergaed_G_clean, wp+\"graphs/tem_all_graph_merge_clean.graphml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf0f71d",
   "metadata": {},
   "source": [
    "### Get minimal path cover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "7c032c2e-34f6-46a0-9723-adae7b8cf211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path 1: [cellulitis, nonradiating, no syncope, dyspnea, chest twinges, Her lower extremity edema, Her shortness of breath, sharp, short of breath, no nausea, no sweating, lower extremity edema, no vomiting] ---> [several years:P3Y, a few seconds:PT5S, several years:P3Y] ---> [the several weeks:P3W] ---> [02/01/2002:2002-02-01]\n",
      "Path 2: [osteoarthritis] ---> [no chills, no fevers, Her shortness of breath, orthopnea, an associated dry cough, dyspnea, no leg pain, increased shortness of breath, no noparoxysmal nocturnal dyspnea] ---> [2 1/2 years:P2.5.Y, 2-3 years:P2.5Y, 5 days:P5D] ---> [02/01/2002:2002-02-01]\n",
      "Path 3: [osteoarthritis] ---> [a cane, walker] ---> [02/01/2002:2002-02-01]\n",
      "Path 4: [no diarrhea, no vision changes, no melena, no hematochezia, no dyspnea, no tingling, no abdominal pain, no numbness, no headaches, no hematuria] ---> [02/01/2002:2002-02-01]\n",
      "Path 5: [light headed] ---> [cellulitis, nonradiating, no syncope, dyspnea, chest twinges, Her lower extremity edema, Her shortness of breath, sharp, short of breath, no nausea, no sweating, lower extremity edema, no vomiting] ---> [several years:P3Y, a few seconds:PT5S, several years:P3Y]\n",
      "Path 6: [obesity, hypertension] ---> [02/01/2002:2002-02-01]\n",
      "Path 7: [no any pleural chest pain] ---> [02/01/2002:2002-02-01]\n"
     ]
    }
   ],
   "source": [
    "covering_paths = minimal_path_cover(mergaed_G_clean)\n",
    "G = mergaed_G_clean.copy()\n",
    "all_times = []\n",
    "path_desc = []\n",
    "# print(\"Minimal path cover:\")\n",
    "for i, path in enumerate(covering_paths):\n",
    "    # print(f\"Path {i+1}: {path}\")\n",
    "    texts = []\n",
    "    for node in path:\n",
    "        text = G.nodes[node].get(\"text\", None)\n",
    "        type = G.nodes[node].get(\"type\", None)\n",
    "        norm_time = G.nodes[node].get('norm_time', None)\n",
    "        # print(node, G.nodes[node])\n",
    "        \n",
    "        if type == None:\n",
    "            elist, tlist = [], []\n",
    "            type_attrs = G.nodes[node].get(\"node_attributes\")\n",
    "            type_attrs = json.loads(type_attrs)\n",
    "            for nid in type_attrs.keys():\n",
    "                # print(nid, type_attrs[nid])\n",
    "                text = type_attrs[nid]['text']\n",
    "                type = type_attrs[nid]['type']\n",
    "                norm_time = type_attrs[nid].get('norm_time', None)\n",
    "                \n",
    "                if type in ['PROBLEM', 'TEST', 'TREATMENT']:\n",
    "                    elist.append(text)\n",
    "                elif type in ['DATE', 'DURATION', 'FREQUENCY', 'TIME']:\n",
    "                    tlist.append(text+':'+norm_time)\n",
    "                    all_times.append([nid, text, norm_time])\n",
    "            if len(elist) > 0:\n",
    "                texts.append( '[' + ', '.join(elist) + ']')\n",
    "            if len(tlist) > 0:\n",
    "                texts.append( '[' + ', '.join(tlist) + ']')\n",
    "        else:\n",
    "            if type in ['PROBLEM', 'TEST', 'TREATMENT']:\n",
    "                texts.append('[' + text + ']')\n",
    "            if type in ['DATE', 'DURATION', 'FREQUENCY', 'TIME']:\n",
    "                texts.append('[' + text+':'+ norm_time + ']')\n",
    "                all_times.append([node, text, norm_time])\n",
    "    # print(texts)\n",
    "    \n",
    "    path_desc.append(f\"Path {i+1}: \" + ' ---> '.join(texts)) \n",
    "print('\\n'.join(path_desc))\n",
    "min_paths = '\\n'.join(path_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "f6853136-da9b-464d-858d-f651779248b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T7</td>\n",
       "      <td>several years</td>\n",
       "      <td>P3Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T6</td>\n",
       "      <td>a few seconds</td>\n",
       "      <td>PT5S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T4</td>\n",
       "      <td>several years</td>\n",
       "      <td>P3Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T8</td>\n",
       "      <td>the several weeks</td>\n",
       "      <td>P3W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T0</td>\n",
       "      <td>02/01/2002</td>\n",
       "      <td>2002-02-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>T5</td>\n",
       "      <td>2 1/2 years</td>\n",
       "      <td>P2.5.Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>T3</td>\n",
       "      <td>2-3 years</td>\n",
       "      <td>P2.5Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>T2</td>\n",
       "      <td>5 days</td>\n",
       "      <td>P5D</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id               text         val\n",
       "0  T7      several years         P3Y\n",
       "1  T6      a few seconds        PT5S\n",
       "2  T4      several years         P3Y\n",
       "3  T8  the several weeks         P3W\n",
       "4  T0         02/01/2002  2002-02-01\n",
       "5  T5        2 1/2 years      P2.5.Y\n",
       "6  T3          2-3 years       P2.5Y\n",
       "7  T2             5 days         P5D"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_times = pd.DataFrame(all_times, columns=['id', 'text', 'val'])\n",
    "all_times = all_times.drop_duplicates()\n",
    "all_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "0b1391e1-a684-48b9-b4d2-0d47c9ea174b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id               text         val    new_text\n",
      "0  T7      several years         P3Y     3 years\n",
      "1  T6      a few seconds        PT5S   5 seconds\n",
      "2  T4      several years         P3Y     3 years\n",
      "3  T8  the several weeks         P3W     3 weeks\n",
      "4  T0         02/01/2002  2002-02-01  2002-02-01\n",
      "5  T5        2 1/2 years      P2.5.Y   2.5 years\n",
      "6  T3          2-3 years       P2.5Y   2.5 years\n",
      "7  T2             5 days         P5D      5 days\n"
     ]
    }
   ],
   "source": [
    "# Create a new column replacing the vague text with the extracted number and unit from 'val'\n",
    "all_times['new_text'] = all_times['val'].apply(extract_duration)\n",
    "\n",
    "# Display the original and transformed text values\n",
    "print(all_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d4b36c-6b6a-454d-a386-6941abd4cc00",
   "metadata": {},
   "source": [
    "- TODO summarize path based on context\n",
    "- Cannot simple comabine all relative time together as duration, as they can related to an absolute date or themseleves. \n",
    "- Some duration are about time duration related to a reference date, it is just duration, They last a few seconds on the left side.\n",
    "- One cannot use LLM to gerenate gold standard and evaluate LLM's performance based on the dataset\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5542d5c8-712e-4154-91a8-f3081ee8d2b9",
   "metadata": {},
   "source": [
    "### LLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "1f36f08c-b34b-4547-adc2-2dea047c293a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from lmformatenforcer import JsonSchemaParser\n",
    "from pydantic import BaseModel\n",
    "import re, json, os\n",
    "from typing import Optional, Type, TypeVar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "ef6ba83d-2342-4812-89d1-2fb2fb44a6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify OpenAI's API key and API base to use vLLM's API server.\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://host.docker.internal:8000/v1\"\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "9e0c74fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf4ebe1-7290-459b-8ba1-372141a07bd9",
   "metadata": {},
   "source": [
    "### Infer relative expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "daace2e9-56f8-4024-a153-b71e1c843450",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_duration(val):\n",
    "    \"\"\"\n",
    "    Extract numeric value, time unit, and special label for frequency expressions from a duration string.\n",
    "    \n",
    "    Expected formats (ignoring case and stray dots):\n",
    "      - ISO 8601 style, e.g. \"P3Y\", \"PT5S\", \"P3W\", \"P2.5Y\", etc.\n",
    "      - Frequency expressions, e.g. \"rp24h\" which should be interpreted as \"repeat period 24 hours\"\n",
    "      \n",
    "    If the string doesn't match a duration pattern, return it as-is.\n",
    "    \"\"\"\n",
    "    unit_mapping = {\n",
    "        'Y': \"years\",\n",
    "        'M': \"months\",\n",
    "        'W': \"weeks\",\n",
    "        'D': \"days\",\n",
    "        'H': \"hours\",\n",
    "        'S': \"seconds\"\n",
    "    }\n",
    "    \n",
    "    # Handle frequency expressions that start with \"rp\" (e.g. \"rp24h\")\n",
    "    if val.lower().startswith(\"rp\"):\n",
    "        pattern_rp = re.compile(r'^rp(\\d+(?:\\.\\d+)?)([smhdwy])$', re.IGNORECASE)\n",
    "        match_rp = pattern_rp.match(val)\n",
    "        if match_rp:\n",
    "            number = match_rp.group(1)\n",
    "            unit_letter = match_rp.group(2).upper()\n",
    "            unit = unit_mapping.get(unit_letter, unit_letter)\n",
    "            return f\"repeat period {number} {unit}\"\n",
    "    \n",
    "    # Remove stray dots that immediately precede a unit letter for ISO 8601 durations.\n",
    "    clean_val = re.sub(r'\\.([YMWDHS])', r'\\1', val)\n",
    "    \n",
    "    # Regex pattern for ISO 8601 style durations.\n",
    "    # Matches strings like \"P3Y\", \"PT5S\", \"P3W\", \"P2.5Y\", etc.\n",
    "    pattern = re.compile(r'P(?:T)?([\\d]+(?:\\.[\\d]+)?)([YMWDHS])', re.IGNORECASE)\n",
    "    match = pattern.match(clean_val)\n",
    "    \n",
    "    if match:\n",
    "        # Extract the numeric value and unit letter (uppercased for mapping).\n",
    "        number = match.group(1)\n",
    "        unit_letter = match.group(2).upper()\n",
    "        unit = unit_mapping.get(unit_letter, unit_letter)\n",
    "        return f\"{number} {unit}\"\n",
    "    else:\n",
    "        # If no valid duration is found, return the original string.\n",
    "        return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "95dc6ce6-c285-46ad-afe2-621026c055a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_boundary_newlines(text: str, start: int, end: int):\n",
    "    \"\"\"\n",
    "    Given a text and a cutoff range (start, end), this function finds:\n",
    "      - The index of the first newline before the start position (or the beginning of the text if none is found)\n",
    "      - The index of the first newline after the end position (or the end of the text if none is found)\n",
    "    \n",
    "    Args:\n",
    "        text (str): The input text to search within.\n",
    "        start (int): The starting index of the cutoff range.\n",
    "        end (int): The ending index of the cutoff range.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: A tuple (before, after) where:\n",
    "            - before (int) is the index right after the newline before 'start' (or 0 if not found),\n",
    "            - after (int) is the index of the newline after 'end' (or len(text) if not found).\n",
    "    \n",
    "    Example:\n",
    "        text = \"Line one.\\nLine two.\\nLine three.\"\n",
    "        start = 10  # Some index within \"Line two.\"\n",
    "        end = 20    # Some further index within \"Line two.\"\n",
    "        \n",
    "        The function will return the indices corresponding to the boundaries\n",
    "        around \"Line two.\".\n",
    "    \"\"\"\n",
    "    # Find the index of the newline that occurs before 'start'\n",
    "    before = text.rfind('\\n', 0, start)\n",
    "    if before == -1:\n",
    "        before = 0  # If no newline is found, start at the beginning of the text\n",
    "    else:\n",
    "        before += 1  # Start just after the newline\n",
    "    \n",
    "    # Find the index of the newline that occurs after 'end'\n",
    "    after = text.find('\\n', end)\n",
    "    if after == -1:\n",
    "        after = len(text)  # If no newline is found, go to the end of the text\n",
    "    \n",
    "    return text[before: after]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "c50abbd8-20f0-4f85-8c02-af938c77a765",
   "metadata": {},
   "outputs": [],
   "source": [
    "def section_time(doc_id, data, section='all'):\n",
    "    text = data[doc_id]['ClinicalNarrativeTemporalAnnotation']['TEXT']\n",
    "    \n",
    "    history_start = text.index('HISTORY OF PRESENT ILLNESS ')\n",
    "    substrings = ['HOSPITAL COURSE']\n",
    "    history_end = find_first_regex(text, substrings)\n",
    "\n",
    "    sect_start, sect_end = 0, len(text)\n",
    "    if section == 'history':\n",
    "        sect_start, sect_end = history_start, history_end\n",
    "    elif section == 'other':\n",
    "        sect_start, sect_end = history_end, len(text)\n",
    "    # print(text[sect_start:sect_end])\n",
    "    \n",
    "    # Remove duplicated admission and discharge time.\n",
    "    # adm_dis = pd.DataFrame(data[doc_id]['ClinicalNarrativeTemporalAnnotation']['TAGS']['SECTIME'])\n",
    "    times = pd.DataFrame(data[doc_id]['ClinicalNarrativeTemporalAnnotation']['TAGS']['TIMEX3'])\n",
    "    times['start'] = times['start'].astype(int)\n",
    "    times['end'] = times['end'].astype(int)\n",
    "    if section == 'history':\n",
    "        times = times.loc[((times['start']>=sect_start) & (times['end']<=sect_end)) | (times['id']=='T0')]\n",
    "    elif section == 'other':\n",
    "        times = times.loc[((times['start']>=sect_start) & (times['end']<=sect_end))| (times['id']=='T1')]\n",
    "    else:\n",
    "        times = times.loc[((times['start']>=sect_start) & (times['end']<=sect_end))| (times['id'].isin(['T0', 'T1']))]\n",
    "\n",
    "    times['new_text'] = times['val'].apply(extract_duration)\n",
    "    times['sentence'] = times.apply(lambda row: find_boundary_newlines(text, row['start'], row['end']), axis=1)\n",
    "    return times, text[:history_start] + text[sect_start:sect_end]\n",
    "    \n",
    "    # return G, text[sect_start:sect_end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "9ba42a2d-a3d3-44fa-975b-be087969e30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "times, context = section_time('36.xml', train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "43b41f2f-52dc-44f7-96ed-7548c1ecd9e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>val</th>\n",
       "      <th>mod</th>\n",
       "      <th>new_text</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T7</td>\n",
       "      <td>1129</td>\n",
       "      <td>1142</td>\n",
       "      <td>several years</td>\n",
       "      <td>DURATION</td>\n",
       "      <td>P3Y</td>\n",
       "      <td>APPROX</td>\n",
       "      <td>3 years</td>\n",
       "      <td>She has had lower extremity edema for thelast ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T8</td>\n",
       "      <td>1226</td>\n",
       "      <td>1243</td>\n",
       "      <td>the several weeks</td>\n",
       "      <td>DURATION</td>\n",
       "      <td>P3W</td>\n",
       "      <td>APPROX</td>\n",
       "      <td>3 weeks</td>\n",
       "      <td>Her lower extremity edema has increased for th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T0</td>\n",
       "      <td>18</td>\n",
       "      <td>28</td>\n",
       "      <td>02/01/2002</td>\n",
       "      <td>DATE</td>\n",
       "      <td>2002-02-01</td>\n",
       "      <td>NA</td>\n",
       "      <td>2002-02-01</td>\n",
       "      <td>02/01/2002\\nDischarge Date :</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T9</td>\n",
       "      <td>2006</td>\n",
       "      <td>2018</td>\n",
       "      <td>several days</td>\n",
       "      <td>DURATION</td>\n",
       "      <td>P3D</td>\n",
       "      <td>APPROX</td>\n",
       "      <td>3 days</td>\n",
       "      <td>On admission , she was diuresed with Lasix and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T10</td>\n",
       "      <td>2128</td>\n",
       "      <td>2133</td>\n",
       "      <td>q day</td>\n",
       "      <td>FREQUENCY</td>\n",
       "      <td>rp24h</td>\n",
       "      <td>NA</td>\n",
       "      <td>repeat period 24 hours</td>\n",
       "      <td>She was restarted on 20 mg p.o. q day prior to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>T2</td>\n",
       "      <td>216</td>\n",
       "      <td>222</td>\n",
       "      <td>5 days</td>\n",
       "      <td>DURATION</td>\n",
       "      <td>P5D</td>\n",
       "      <td>NA</td>\n",
       "      <td>5 days</td>\n",
       "      <td>Saujule Study is a 77-year-old woman with a hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>T3</td>\n",
       "      <td>284</td>\n",
       "      <td>293</td>\n",
       "      <td>2-3 years</td>\n",
       "      <td>DURATION</td>\n",
       "      <td>P2.5Y</td>\n",
       "      <td>APPROX</td>\n",
       "      <td>2.5 years</td>\n",
       "      <td>Her shortness of breath has been progressive o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>T12</td>\n",
       "      <td>3266</td>\n",
       "      <td>3272</td>\n",
       "      <td>t.i.d.</td>\n",
       "      <td>FREQUENCY</td>\n",
       "      <td>rp8h</td>\n",
       "      <td>NA</td>\n",
       "      <td>repeat period 8 hours</td>\n",
       "      <td>Her blood pressure was high so Captopril was t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>T13</td>\n",
       "      <td>3395</td>\n",
       "      <td>3400</td>\n",
       "      <td>q day</td>\n",
       "      <td>FREQUENCY</td>\n",
       "      <td>rp24h</td>\n",
       "      <td>NA</td>\n",
       "      <td>repeat period 24 hours</td>\n",
       "      <td>She is on hydrochlorothiazide 50 mg p.o. q day .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>T15</td>\n",
       "      <td>4120</td>\n",
       "      <td>4128</td>\n",
       "      <td>3 nights</td>\n",
       "      <td>DURATION</td>\n",
       "      <td>PT36H</td>\n",
       "      <td>NA</td>\n",
       "      <td>36 hours</td>\n",
       "      <td>BIPAP was attempted for 3 nights in a row and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>T1</td>\n",
       "      <td>46</td>\n",
       "      <td>56</td>\n",
       "      <td>02/08/2002</td>\n",
       "      <td>DATE</td>\n",
       "      <td>2002-02-08</td>\n",
       "      <td>NA</td>\n",
       "      <td>2002-02-08</td>\n",
       "      <td>02/08/2002\\nHISTORY OF PRESENT ILLNESS :</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>T16</td>\n",
       "      <td>4682</td>\n",
       "      <td>4694</td>\n",
       "      <td>10 more days</td>\n",
       "      <td>DURATION</td>\n",
       "      <td>P10D</td>\n",
       "      <td>APPROX</td>\n",
       "      <td>10 days</td>\n",
       "      <td>She will need to continue for at least 10 more...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>T18</td>\n",
       "      <td>5209</td>\n",
       "      <td>5214</td>\n",
       "      <td>q day</td>\n",
       "      <td>FREQUENCY</td>\n",
       "      <td>rp24h</td>\n",
       "      <td>NA</td>\n",
       "      <td>repeat period 24 hours</td>\n",
       "      <td>Her dose was increased to 7.5 mg q day the day...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>T17</td>\n",
       "      <td>5215</td>\n",
       "      <td>5241</td>\n",
       "      <td>the day prior to discharge</td>\n",
       "      <td>DATE</td>\n",
       "      <td></td>\n",
       "      <td>NA</td>\n",
       "      <td></td>\n",
       "      <td>Her dose was increased to 7.5 mg q day the day...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>T19</td>\n",
       "      <td>5295</td>\n",
       "      <td>5300</td>\n",
       "      <td>1 day</td>\n",
       "      <td>DURATION</td>\n",
       "      <td>p1d</td>\n",
       "      <td>NA</td>\n",
       "      <td>1 days</td>\n",
       "      <td>She will be discharged on this dose in additio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>T4</td>\n",
       "      <td>697</td>\n",
       "      <td>710</td>\n",
       "      <td>several years</td>\n",
       "      <td>DURATION</td>\n",
       "      <td>P3Y</td>\n",
       "      <td>APPROX</td>\n",
       "      <td>3 years</td>\n",
       "      <td>Her shortness of breath and dyspnea on exertio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>T5</td>\n",
       "      <td>791</td>\n",
       "      <td>802</td>\n",
       "      <td>2 1/2 years</td>\n",
       "      <td>DURATION</td>\n",
       "      <td>P2.5.Y</td>\n",
       "      <td>NA</td>\n",
       "      <td>2.5 years</td>\n",
       "      <td>She sleeps in a chair up right for the last 2 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>T6</td>\n",
       "      <td>982</td>\n",
       "      <td>995</td>\n",
       "      <td>a few seconds</td>\n",
       "      <td>DURATION</td>\n",
       "      <td>PT5S</td>\n",
       "      <td>APPROX</td>\n",
       "      <td>5 seconds</td>\n",
       "      <td>They last a few seconds on the left side and a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  start   end                        text       type         val  \\\n",
       "0    T7   1129  1142               several years   DURATION         P3Y   \n",
       "1    T8   1226  1243           the several weeks   DURATION         P3W   \n",
       "2    T0     18    28                  02/01/2002       DATE  2002-02-01   \n",
       "3    T9   2006  2018                several days   DURATION         P3D   \n",
       "4   T10   2128  2133                       q day  FREQUENCY       rp24h   \n",
       "5    T2    216   222                      5 days   DURATION         P5D   \n",
       "6    T3    284   293                   2-3 years   DURATION       P2.5Y   \n",
       "7   T12   3266  3272                      t.i.d.  FREQUENCY        rp8h   \n",
       "8   T13   3395  3400                       q day  FREQUENCY       rp24h   \n",
       "9   T15   4120  4128                    3 nights   DURATION       PT36H   \n",
       "10   T1     46    56                  02/08/2002       DATE  2002-02-08   \n",
       "11  T16   4682  4694                10 more days   DURATION        P10D   \n",
       "12  T18   5209  5214                       q day  FREQUENCY       rp24h   \n",
       "13  T17   5215  5241  the day prior to discharge       DATE               \n",
       "14  T19   5295  5300                       1 day   DURATION         p1d   \n",
       "15   T4    697   710               several years   DURATION         P3Y   \n",
       "16   T5    791   802                 2 1/2 years   DURATION      P2.5.Y   \n",
       "17   T6    982   995               a few seconds   DURATION        PT5S   \n",
       "\n",
       "       mod                new_text  \\\n",
       "0   APPROX                 3 years   \n",
       "1   APPROX                 3 weeks   \n",
       "2       NA              2002-02-01   \n",
       "3   APPROX                  3 days   \n",
       "4       NA  repeat period 24 hours   \n",
       "5       NA                  5 days   \n",
       "6   APPROX               2.5 years   \n",
       "7       NA   repeat period 8 hours   \n",
       "8       NA  repeat period 24 hours   \n",
       "9       NA                36 hours   \n",
       "10      NA              2002-02-08   \n",
       "11  APPROX                 10 days   \n",
       "12      NA  repeat period 24 hours   \n",
       "13      NA                           \n",
       "14      NA                  1 days   \n",
       "15  APPROX                 3 years   \n",
       "16      NA               2.5 years   \n",
       "17  APPROX               5 seconds   \n",
       "\n",
       "                                             sentence  \n",
       "0   She has had lower extremity edema for thelast ...  \n",
       "1   Her lower extremity edema has increased for th...  \n",
       "2                        02/01/2002\\nDischarge Date :  \n",
       "3   On admission , she was diuresed with Lasix and...  \n",
       "4   She was restarted on 20 mg p.o. q day prior to...  \n",
       "5   Saujule Study is a 77-year-old woman with a hi...  \n",
       "6   Her shortness of breath has been progressive o...  \n",
       "7   Her blood pressure was high so Captopril was t...  \n",
       "8    She is on hydrochlorothiazide 50 mg p.o. q day .  \n",
       "9   BIPAP was attempted for 3 nights in a row and ...  \n",
       "10           02/08/2002\\nHISTORY OF PRESENT ILLNESS :  \n",
       "11  She will need to continue for at least 10 more...  \n",
       "12  Her dose was increased to 7.5 mg q day the day...  \n",
       "13  Her dose was increased to 7.5 mg q day the day...  \n",
       "14  She will be discharged on this dose in additio...  \n",
       "15  Her shortness of breath and dyspnea on exertio...  \n",
       "16  She sleeps in a chair up right for the last 2 ...  \n",
       "17  They last a few seconds on the left side and a...  "
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "fbf20838-c1ee-4412-90f5-eed012aa0ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the several weeks 3 weeks Her lower extremity edema has increased for the several weeks prior to admission secondary to an inability to elevate her legs due to a broken chair at home .\n"
     ]
    }
   ],
   "source": [
    "row = times.iloc[1]\n",
    "time_exp, new_text, sentence = row['text'], row['new_text'], row['sentence']\n",
    "print(time_exp, new_text, sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "6d2c8cdd-0b6f-48b1-91e6-f3cf310f85a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, let's tackle this problem step by step. The user is asking to determine the exact date corresponding to the time expression \"the several weeks\" in the given context. The example provided in the question uses an anchor date and works backward, so I need to apply the same logic here.\n",
      "\n",
      "First, I need to parse the context given. The Admission Date is 02/01/2002, and the Discharge Date is 02/08/2002. The relevant part of the HISTORY OF PRESENT ILLNESS mentions that the patient's lower extremity edema has increased for the past several weeks prior to admission. The anchor phrase specifically states this increase happened \"several weeks prior to admission,\" so the anchor date here is the Admission Date, which is February 1, 2002.\n",
      "\n",
      "The time expression given is \"the several weeks,\" and the approximate time is 3 weeks. Since the phrase says the edema increased for several weeks before admission, we need to count backward from the admission date by approximately 3 weeks. \n",
      "\n",
      "Three weeks prior to February 1 would be January 11, 2002 (since 2 weeks back would be January 18, and adding another week gets to January 11). But here's the catch: when the problem mentions \"for the several weeks prior to admission,\" it's a period ending at the admission date. So the start date would be three weeks before admission, and the end would be the admission date itself. However, the problem says the lower extremity edema increased \"for the several weeks prior,\" meaning the increase started 3 weeks before admission and continued up until admission. \n",
      "\n",
      "Therefore, the start date would be 3 weeks before 2002-02-01. Let me calculate that:\n",
      "\n",
      "Starting from February 1, subtracting 3 weeks (21 days). Let's do the math step by step.\n",
      "\n",
      "February 1 minus 21 days: January has 31 days, so subtracting 21 from February 1 would take us back into January. Let's see:\n",
      "\n",
      "31 days in January. So February 1 is day 32 of the year 2002 (since 2002 is not a leap year). Subtracting 21 days from February 1 (32 - 21 = 11) would give January 11. Therefore, three weeks prior is January 11. So the period would be from January 11 to February 1. Since the time expression is a period (the several weeks before admission), the start is January 11 and the end is the admission date. The question asks for exact dates, so I need to format that into a start and end in the JSON.\n",
      "\n",
      "Wait, but in the example provided earlier, when the anchor was \"several days before admission\" (3 days approx), the output was a single date (2013-07-24), which is 3 days before the admission date (2013-07-27). So the example's time expression was a point in time (the days before admission, ending at admission day minus). But in our current case, the time expression is a period of \"several weeks\" leading up to the admission. The question says to return either a single date or a date range. Since this is a range, we should use the start and end dates.\n",
      "\n",
      "Therefore, the start date is three weeks before admission, and the end date is the admission date. So the start would be 2002-01-11 and the end 2002-02-01. However, I need to confirm the exact dates again. Let me verify the subtraction:\n",
      "\n",
      "From February 1, 2002, going back 3 weeks:\n",
      "\n",
      "- 1 week back: January 25 (since February has 28 days in 2002, non-leap year)\n",
      "- 2 weeks back: January 18\n",
      "- 3 weeks back: January 11\n",
      "\n",
      "Yes, that's correct. So the period starts on January 11 and ends on February 1. Thus the JSON should have start as 2002-01-11 and end as 2002-02-01. \n",
      "\n",
      "Wait, but sometimes when people say \"prior to admission,\" the period ends the day before admission. However in this context, the edema increased for several weeks prior, meaning up until the admission. So admission date is included. \n",
      "\n",
      "Therefore, the correct JSON should be {\"start\": \"2002-01-11\", \"end\": \"2002-02-01\"}\n",
      "</think>\n",
      "\n",
      "{\"start\": \"2002-01-11\", \"end\": \"2002-02-01\"}\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"Using the given anchor date along with its context, determine the exact date that corresponds to the provided time expression. For example:\n",
    "\n",
    "Context: The patient was admitted on 2013-07-27. He had experienced a cold several days before admission and stayed in the hospital for two weeks.\n",
    "Time expression: several days\n",
    "Approximate time: 3 days\n",
    "Anchor phrase: several days before admission\n",
    "Output: 2013-07-24\n",
    "\n",
    "Now, given:\n",
    "Context: {context}\n",
    "Time expression: {time_exp}\n",
    "Approximate time: {new_text}\n",
    "Anchor phrase: {sentence}\n",
    "Output:\n",
    "\n",
    "Please provide your answer strictly in JSON format. If the time expression corresponds to a single date, return:\n",
    "{{\"result\": \"YYYY-MM-DD\"}}\n",
    "If it represents a date range, return:\n",
    "{{\"start\": \"YYYY-MM-DD\", \"end\": \"YYYY-MM-DD\"}}\n",
    "\"\"\"\n",
    "\n",
    "chat_response = client.chat.completions.create(\n",
    "    model = 'Qwen/QwQ-32B-AWQ',\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    ")\n",
    "response_text = chat_response.choices[0].message.content\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "ea6b2d58-b131-42ce-8718-618169b9dfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import VLLMOpenAI\n",
    "from pydantic import BaseModel\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "11e1581b-7095-46a1-8790-c47ad0ff13a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://host.docker.internal:8000/v1\"\n",
    "model_name = 'Qwen/QwQ-32B-AWQ'\n",
    "\n",
    "llm = VLLMOpenAI(\n",
    "    openai_api_key=openai_api_key,\n",
    "    openai_api_base=openai_api_base,\n",
    "    model_name=model_name,\n",
    "    # model_kwargs={\"stop\": [\".\"]},\n",
    ")\n",
    "# print(llm.invoke(\"Rome is\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "54a77400-9f5f-4b17-9b49-ef6d45c71e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First, let's parse the problem step by step.\n",
      "\n",
      "The task is to determine the exact date corresponding to the time expression \"the several weeks\" as mentioned in the anchor phrase. The context provides an admission date of 02/01/2002 and a discharge date of 02/08/2002. \n",
      "\n",
      "The anchor phrase states: \"Her lower extremity edema has increased for the several weeks prior to admission secondary to an inability to elevate her legs due to a broken chair at home.\"\n",
      "\n",
      "The time expression here is \"the several weeks,\" and the approximate time given is 3 weeks. The task is to calculate the start and end dates for this period leading up to admission.\n",
      "\n",
      "Since the admission date is 02/01/2002, the time expression refers to the period before admission. The phrase \"prior to admission\" indicates that the entire period is before 02/01/2002. The time expression \"several weeks\" is approximated as 3 weeks. \n",
      "\n",
      "Now, \"prior to admission\" means the end of the time period is the day before admission (since the admission date is the start of the hospital stay). Therefore, the end date should be\n"
     ]
    }
   ],
   "source": [
    "class DateRangeAnswer(BaseModel):\n",
    "    \"\"\"\n",
    "    Pydantic model for board response with required source and evidence fields,\n",
    "    and optional string fields for board-related information\n",
    "    \"\"\"\n",
    "    start: str = Field(description=\"ISO formatted start date (YYYY-MM-DD)\")\n",
    "    end: str = Field(description=\"ISO formatted end date (YYYY-MM-DD)\")\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=DateRangeAnswer)    \n",
    "\n",
    "        \n",
    "# chain = (llm | StrOutputParser() | parser)\n",
    "chain = (llm)\n",
    "prompt = f\"\"\"Using the given anchor date along with its context, determine the exact date that corresponds to the provided time expression. For example:\n",
    "\n",
    "Context: The patient was admitted on 2013-07-27. He had experienced a cold several days before admission and stayed in the hospital for two weeks.\n",
    "Time expression: several days\n",
    "Approximate time: 3 days\n",
    "Anchor phrase: several days before admission\n",
    "Output: {{\"start\": \"2013-07-24\", \"end\": \"2013-07-27\"}}\n",
    "\n",
    "Now, given:\n",
    "Context: {context}\n",
    "Time expression: {time_exp}\n",
    "Approximate time: {new_text}\n",
    "Anchor phrase: {sentence}\n",
    "Output:\n",
    "\"\"\"\n",
    "# print(prompt)\n",
    "response = chain.invoke(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971fd1db-52aa-45f2-8689-8badf7ba1601",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feadf709-0769-46b2-91e2-2207888f14b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f379a3-2603-4b41-bd1d-a4421063f342",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4be298-5385-4757-b223-c76d28b79673",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d7dcfd4-a5c8-4194-b127-bdd019bbc6be",
   "metadata": {},
   "source": [
    "### Prompting timeline building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f86eb1-74df-4e5e-a84d-e33d28f9b5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## prompt = \"\"\"Construct a timeline from text \"Admission Date : 02/01/2002 HISTORY OF PRESENT ILLNESS : Saujule Study is a 77-year-old woman with a history of obesity and hypertension who presents with increased shortness of breath x 5 days . Her shortness of breath has been progressive over the last 2-3 years . She has an associated dry cough but no fevers , chills , or leg pain . She has dyspnea on exertion . She ambulates with walker and a cane secondary to osteoarthritis . She becomes short of breath just by getting up from her chair and can only walk 2-3 steps on a flat surface . She feels light headed when getting up . Her shortness of breath and dyspnea on exertion has been progressive for the past several years . It has not been sudden or acute . She sleeps in a chair up right for the last 2 1/2 years secondary to osteoarthritis . She has orthopnea as well but noparoxysmal nocturnal dyspnea . She occasionally feels chest twinges which are nonradiating but are sharp . They last a few seconds on the left side and are not associated with sweating , nausea , vomiting or syncope . She has had lower extremity edema for thelast several years with multiple episodes of cellulitis . Her lower extremity edema has increased for the several weeks prior to admission secondary to an inability to elevate her legs due to a broken chair at home . She denies any pleural chest pain . \n",
    "# REVIEW OF SYSTEMS : Negative for headaches , vision changes , numbness , tingling , dyspnea , hematuria , abdominal pain , diarrhea , melena or hematochezia .\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "eec2b3f5-7d7b-4fa6-a723-20d02976580b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat response: Okay, let's tackle this timeline construction from the provided medical text. Hmm, the user wants a timeline based on Saujule Study's medical history. First, I need to parse through the given information and identify all the key events and their dates. \n",
      "\n",
      "Starting with the Admission Date: that's 02/01/2002. That's the starting point. \n",
      "\n",
      "Now, the History of Present Illness section. She's 77, so I need to figure out the ages or times when the symptoms started. Let's see: her shortness of breath has been progressive over 2-3 years. Since she was 77 at admission (on 2002), going back 2-3 years would be around age 74 to 75 years. Wait, but 2002 minus 3 years would be 1999, so age 74 (77 -3). So that's an ongoing issue.\n",
      "\n",
      "The increased shortness of breath has been over the last 2-3 years, but it specifically mentions \"increased shortness of breath x 5 days\" which likely means in the five days before admission. So the worsening in the past five days is a key point. \n",
      "\n",
      "She has a dry cough but no fevers, chills, or leg pain. The dyspnea on exertion is a chronic issue, as her being able to only walk a few steps on a flat surface indicates long-term progression. \n",
      "\n",
      "The orthopnea and sleep in an upright position for 2 1/2 years. Since she was admitted in 2002, that would be since around mid-1999 or so. \n",
      "\n",
      "The chest twinges: these are intermittent but not associated with other symptoms, so maybe not a significant timeline item unless there's a specific timeframe given. The text just says \"occasionally feels\", no specific starting point.\n",
      "\n",
      "Lower extremity edema for several years, with episodes of cellulitis. The edema increased in the several weeks before admission due to a broken chair. So the broken chair is a recent event (several weeks before 02/01/2002). Also, she can't elevate her legs because of the broken chair leading to worse edema.\n",
      "\n",
      "Denies pleural chest pain, and the other systems in the ROS are negative. \n",
      "\n",
      "Now, I need to structure all these points into a timeline starting from the earliest events to the admission. Let me list each point with their times relative to admission.\n",
      "\n",
      "First, the orthopnea and sleeping upright started 2.5 years before admission. So that would be mid-1999 (since 2002 minus 2.5 years). \n",
      "\n",
      "The progressive dyspnea over the past 2-3 years would start around 1999 as well. \n",
      "\n",
      "The chronic lower extremity edema for several years started maybe 3 years prior, so 1999 again? The increase in edema was in the few weeks before admission (late November/early December 2001). \n",
      "\n",
      "The chest twinges are occasional, so maybe they started when the edema or other symptoms began, but the text doesn't specify, so perhaps just note that they have occurred occasionally but not at a specific time.\n",
      "\n",
      "The exacerbation of shortness of breath (increased) in the last 5 days prior to admission, so that would be late January (1982? Wait, wait, hold on. Wait, the Admission Date is 02/01/2002, right? So the 5 days before would be 1/27/2002 to 1/31/2002. Yes, that makes sense.\n",
      "\n",
      "The broken chair leading to inability to elevate legs causing edema increase occurred in the weeks before admission, so maybe a few weeks prior, like in late November or December 2001.\n",
      "\n",
      "Putting this all together in a timeline from earliest to latest:\n",
      "\n",
      "- 1999 (2-3 years before admission): onset of progressive dyspnea and orthopnea/sleeping upright.\n",
      "\n",
      "Wait, but the exact dates aren't given. Since the date of admission is 2002, years prior would be approximate. We can note events with approximate dates based on the years mentioned.\n",
      "\n",
      "Let me try to structure:\n",
      "\n",
      "Timeline:\n",
      "\n",
      "Admission Date: 02/01/2002\n",
      "\n",
      "Prior Events:\n",
      "\n",
      "- 2.5 years before admission (approx mid-1999): Orthopnea and starts sleeping upright due to osteoarthritis.\n",
      "\n",
      "- 2-3 years before admission (approx late 1999): Progressive shortness of breath becomes more severe (as it was progressing over those years).\n",
      "\n",
      "- Multiple episodes of cellulitis occurring alongside the chronic lower extremity edema; however, the exact start isn't specified, but since it's noted as \"for the last several years,\" that would align with the 3 years or so before admission.\n",
      "\n",
      "- Within the past several weeks before admission (November 2001 - early January 2002): Lower extremity edema worsened due to broken chair (inability to elevate legs).\n",
      "\n",
      "- 5 days before admission (January 27-31, 的性格可变性这可能的意思是2002): Increased shortness of breath, which prompts the admission.\n",
      "\n",
      "Additionally, the dyspnea on exertion and use of walker/cane are due to osteoarthritis, but those are ongoing and not time-specific.\n",
      "\n",
      "Chest twinges occasional over time, but no specific dates.\n",
      "\n",
      "The user may want the timeline in order from earliest to latest events. So starting with the onset of orthopnea and chronic dyspnea (earliest point), then the increased edema weeks before admission, then the 5-day exacerbation leading to admission.\n",
      "\n",
      "Wait, but the orthopnea was starting 2.5 years before admission, so that's earlier than the 2-3 years of dyspnea. Wait, the text says \"Her shortness of breath has been progressive over the last 2-3 years.\" So the orthopnea is part of that, tied to the same time frame (since her shortness of breath is related to her conditions).\n",
      "\n",
      "But the key is to list all these events in chronological order based on the relative times mentioned. Also, note that some of the symptoms are ongoing and not point-in-time events. \n",
      "\n",
      "Probably the timeline should list:\n",
      "\n",
      "Starting from the earliest possible point based on the 2.5 years (so mid 1999) and the 2-3 years, but we can approximate as \"Several years prior to admission (approx 1999-2000):\"\n",
      "\n",
      "Then the worsening in the several weeks before admission (approx late 2001), leading to the 5 day onset just before admission.\n",
      "\n",
      "Okay, I think that's it. Now I need to present this clearly in bullet points or list form, making sure all key points from the text are included, using the dates and time references from the text.\n",
      "</think>\n",
      "\n",
      "Here’s a timeline constructed based on Saujule Study’s medical history, organized chronologically:\n",
      "\n",
      "---\n",
      "\n",
      "### **Admission Date:** 02/01/2002 (February 1, 2002)\n",
      "\n",
      "---\n",
      "\n",
      "### **Symptoms and Events Leading to Admission:**\n",
      "\n",
      "#### **Several Years Before Admission (Approx. 1999–2000):**\n",
      "- **Onset of Progressive Shortness of Breath:** Began worsening over 2–3 years prior to admission (late 1990s to early 2000s).\n",
      "- **Orthopnea Starts:** Required sleeping upright in a chair for **2½ years** before admission due to osteoarthritis-related discomfort.\n",
      "- **Chronic Lower Extremity Edema:** Developed over \"several years,\" accompanied by recurrent episodes of **cellulitis**.\n",
      "\n",
      "#### **Within the Past Few Weeks Before Admission (Late 2001):**\n",
      "- **Worsening Edema:** Lower extremity swelling intensified due to an inability to elevate her legs. A broken chair at home (event occurring in weeks prior to admission) exacerbated this condition.\n",
      "\n",
      "#### **5 Days Before Admission (Approx. January 27–31, 2002):**\n",
      "- **Increased Shortness of Breath:** Acute exacerbation of dyspnea prompting her urgent medical evaluation. This worsening was noted as a sudden increase in severity over her baseline symptoms.\n",
      "\n",
      "---\n",
      "\n",
      "### **Ongoing Chronic Conditions at Admission:**\n",
      "- **Dyspnea on Exertion:** Extreme limitation (e.g., short of breath getting up from a chair, able to walk only 2–3 steps).\n",
      "- **Orthostatic Symptoms:** Lightheadedness upon standing.\n",
      "- **Functional Limitations:** Uses a walker and cane due to **osteoarthritis**.\n",
      "- **Chest Twinges:** Occasional sharp, nonradiating left-sided twinges (duration: seconds), not associated with cardiac symptoms (e.g., no syncope, sweating).\n",
      "- **Negative Features at Presentation:** No fever, chills, leg pain, paroxysmal nocturnal dyspnea, pleural chest pain, or central nervous system/systemic symptoms.\n",
      "\n",
      "---\n",
      "\n",
      "### **Key Contextual Information:**\n",
      "- **History of:** Obesity, hypertension, osteoarthritis, recurrent cellulitis.\n",
      "- **Denials:** Hematuria, abdominal pain, gastrointestinal bleeding, vision changes, headache, numbness/tingling.\n",
      "\n",
      "---\n",
      "\n",
      "### **Timeline Summary in Chronological Order:**\n",
      "1. **Late 1990s–Early 2000s:**  \n",
      "   - Progressive shortness of breath develops over 2–3 years.  \n",
      "   - Orthopnea begins, necessitating upright sleeping.  \n",
      "   - Chronic lower extremity edema and cellulitis episodes start.  \n",
      "\n",
      "2. **Late 2001 (Several Weeks Before Admission):**  \n",
      "   - Edema worsens due to a broken chair preventing leg elevation.  \n",
      "\n",
      "3. **January 2002 (5 Days Before Admission):**  \n",
      "   - Acute exacerbation of shortness of breath occurs, leading to hospitalization.  \n",
      "\n",
      "4. **Admission Date (02/01/2002):**  \n",
      "   - Formal evaluation and management initiated.\n",
      "\n",
      "---\n",
      "\n",
      "This timeline captures the progression of symptoms, chronic conditions, and precipitating factors leading to her hospitalization.\n"
     ]
    }
   ],
   "source": [
    "chat_response = client.chat.completions.create(\n",
    "    model = 'Qwen/QwQ-32B-AWQ',\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    ")\n",
    "print(\"Chat response:\", chat_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ea3e6dec-19b5-45f4-94d9-0128065d4fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat response: Okay, so I'm trying to construct a timeline from the given medical text about a patient. Let me first read through the text to understand the key events and when they occurred.\n",
      "\n",
      "The admission date is 02/01/2002. That's the starting point. Now, the patient is a 77-year-old woman with a history of obesity and hypertension. She presents with increased shortness of breath for 5 days. But the issue has been progressing over the last 2-3 years. So, I should note that her dyspnea started gradually over several years.\n",
      "\n",
      "She has a dry cough without fevers, chills, or leg pain. She uses a walker and cane due to osteoarthritis, which I think is a chronic issue. She becomes short of breath even when getting up from a chair, which is a recent issue, maybe in the past several weeks or months? Wait, the text says it's been progressive over 2-3 years, so getting breathless upon sitting up is probably part of that gradual progression.\n",
      "\n",
      "She sleeps upright for the last 2.5 years because of osteoarthritis. That's another chronic adjustment. So, that's from 2002 minus 2.5 years, which would be 1999, but wait, she's admitted in 2002, so maybe she started sleeping upright around 1999.\n",
      "\n",
      "She has orthopnea but no paroxysmal nocturnal dyspnea, which is important. She feels sharp chest twinges on the left side, lasting a few seconds, not associated with other symptoms like sweating or syncope. That's more recent, maybe within the past few days or weeks?\n",
      "\n",
      "Lower extremity edema has been present for several years but has increased in the several weeks prior to admission because she couldn't elevate her legs due to a broken chair. So, the edema started a few years back but got worse in the last few weeks.\n",
      "\n",
      "She denied pleural chest pain, so that's negative. In the review of systems, other things are negative. So, the timeline should focus on the progression of her breathing issues and edema.\n",
      "\n",
      "Putting it together, I think the timeline should start with her admission, then go back to the onset of her current symptoms and how they've progressed over the years. Maybe also note when she started using the walker and cane, and when she began sleeping upright. The edema's worsening in the last few weeks is another key point.\n",
      "\n",
      "I need to make sure I list each event with the correct date or timeframe. Since some events are chronic and others are recent, I should structure it so that earlier events come first, with the most recent at the admission date. I should also indicate when her shortness of breath started and how it's gotten worse over time.\n",
      "\n",
      "I think I have all the elements now. Let me structure this into a clear timeline, starting from the earliest events and moving forward to admission.\n",
      "</think>\n",
      "\n",
      "**Timeline of Patient's Conditions Leading to Admission on 02/01/2002**\n",
      "\n",
      "1. **1999:**\n",
      "   - **Sleeping Upright:** The patient began sleeping upright due to osteoarthritis.\n",
      "\n",
      "2. **Around 1999 to 2002:**\n",
      "   - **Osteoarthritis:** The patient started using a walker and cane, indicating the onset or worsening of osteoarthritis.\n",
      "\n",
      "3. **2-3 Years Prior to Admission (circa 1999-2002):**\n",
      "   - **Gradual Onset of Dyspnea:** The patient experienced a progressive increase in shortness of breath over 2-3 years.\n",
      "\n",
      "4. **Several Years Prior to Admission:**\n",
      "   - **Lower Extremity Edema:** The patient developed lower extremity edema, which gradually worsened over time.\n",
      "\n",
      "5. **Within Several Weeks Prior to Admission:**\n",
      "   - **Worsening Edema:** The edema significantly increased due to a broken chair preventing leg elevation.\n",
      "\n",
      "6. **5 Days Prior to Admission:**\n",
      "   - **Increased Shortness of Breath:** The patient reported a noticeable increase in shortness of breath over the last 5 days.\n",
      "\n",
      "7. **Recent Weeks/Months:**\n",
      "   - **Sharp Chest Twinges:** The patient experienced sharp, nonradiating chest twinges on the left side, lasting a few seconds, without associated symptoms like sweating or syncope.\n",
      "\n",
      "8. ** Admission Date: 02/01/2002**\n",
      "   - **Presentation:** The patient was admitted with progressive dyspnea, worsening edema, and other noted symptoms.\n",
      "\n",
      "This timeline outlines the progression of the patient's conditions leading up to her admission, highlighting both chronic and recent developments.\n"
     ]
    }
   ],
   "source": [
    "chat_response = client.chat.completions.create(\n",
    "    model = 'deepseek-ai/DeepSeek-R1-Distill-Qwen-14B',\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    ")\n",
    "print(\"Chat response:\", chat_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7c860b5b-0d67-4aa7-8874-ad425f49522c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat_response = client.chat.completions.create(\n",
    "#     model = 'deepseek-ai/DeepSeek-R1-Distill-Qwen-14B',\n",
    "#     messages=[\n",
    "#         {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "#         {\"role\": \"user\", \"content\": 'what is the date of 5 Days Before 02/01/2002'},\n",
    "#     ]\n",
    "# )\n",
    "# print(\"Chat response:\", chat_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "293a0abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda env list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143b71f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
