{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e3fcf2e-1535-4986-87e5-0b30a15355be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xmltodict, json\n",
    "import os\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from itertools import combinations\n",
    "import json, re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ea9dd3-6fc4-42ac-96b0-af9954baab60",
   "metadata": {},
   "source": [
    "### Load raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ec8452a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pwd\n",
    "# !conda env list\n",
    "# !python --version\n",
    "# !cd /home/jovyan/work/Temporal_relation/\n",
    "# !pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "ceeee538-9c9a-4043-8750-f04b4a692afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to path to the data\n",
    "# path = '/home/wt/Downloads/n2c2 2012/'\n",
    "wp = '/home/jovyan/work/Temporal_relation/'\n",
    "path = wp + 'data/i2b2/'\n",
    "training_data_path = path + 'merge_training'\n",
    "test_data_path = path + 'ground_truth/merged_xml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03b04a12-9b35-446c-8984-8dffb38b6575",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(data_path):\n",
    "    data = {}\n",
    "    for filename in os.listdir(data_path):\n",
    "        if filename.endswith(\".xml\"): \n",
    "            f = (os.path.join(data_path, filename))\n",
    "#             print(f)\n",
    "            fb = open(f, \"rb\").read().decode(encoding=\"utf-8\")\n",
    "#     invalid character '&' https://github.com/martinblech/xmltodict/issues/277\n",
    "            fb = fb.replace('&', '&amp;')\n",
    "            dic = xmltodict.parse(fb, attr_prefix='')\n",
    "#     restore orginal character \"&\"\n",
    "            dic['ClinicalNarrativeTemporalAnnotation']['TEXT'] = dic['ClinicalNarrativeTemporalAnnotation']['TEXT'].replace('&amp;', '&')\n",
    "            data[filename] = (dic)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58df447f-02f8-4e82-a7cc-e48cf806524e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data_loader(training_data_path)\n",
    "test_data = data_loader(test_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f519491c-a84d-4f5f-a9f9-1c2fcbd18ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190 120\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data), len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6fdb414-c896-447c-927b-8cb13664e9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_first_regex(text, substrings):\n",
    "    pattern = '|'.join(map(re.escape, substrings))  # Escape special characters\n",
    "    match = re.search(pattern, text)\n",
    "    if match:\n",
    "        return match.start()\n",
    "    else:\n",
    "        raise ValueError(\"None of the substrings found in the text.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "6a94178c-0404-4aa1-8e70-bf93feaa0b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_section_graph(doc_id, data, section='all'):\n",
    "    # for doc_id in list(data.keys())[:1]:\n",
    "    text = data[doc_id]['ClinicalNarrativeTemporalAnnotation']['TEXT']\n",
    "    # print(text)\n",
    "    \n",
    "    history_start = text.index('HISTORY OF PRESENT ILLNESS ')\n",
    "    substrings = ['REVIEW OF SYSTEMS', 'HOSPITAL COURSE']\n",
    "    history_end = find_first_regex(text, substrings)\n",
    "\n",
    "    sect_start, sect_end = 0, len(text)\n",
    "    if section == 'history':\n",
    "        sect_start, sect_end = history_start, history_end\n",
    "    elif section == 'other':\n",
    "        sect_start, sect_end = history_end, len(text)\n",
    "    # print(text[sect_start:sect_end])\n",
    "\n",
    "    events = pd.DataFrame(data[doc_id]['ClinicalNarrativeTemporalAnnotation']['TAGS']['EVENT'])\n",
    "    events['start'] = events['start'].astype(int)\n",
    "    events['end'] = events['end'].astype(int)\n",
    "    # Filter events in the history section\n",
    "    # print('events', events.shape)\n",
    "    events = events.loc[(events['start']>=sect_start) & (events['end']<=sect_end)]\n",
    "    # print('events after', events.shape)\n",
    "    \n",
    "    # FILTER 1: only use events related to medical concepts\n",
    "    # events = events.loc[events['type'].isin(['PROBLEM', 'TEST', 'TREATMENT'])]\n",
    "    event_types = dict(zip(events['id'], events['type']))\n",
    "    \n",
    "    # Remove duplicated admission and discharge time.\n",
    "    # adm_dis = pd.DataFrame(data[doc_id]['ClinicalNarrativeTemporalAnnotation']['TAGS']['SECTIME'])\n",
    "    times = pd.DataFrame(data[doc_id]['ClinicalNarrativeTemporalAnnotation']['TAGS']['TIMEX3'])\n",
    "    times['start'] = times['start'].astype(int)\n",
    "    times['end'] = times['end'].astype(int)\n",
    "    # print('times', times.shape)\n",
    "    times = times.loc[((times['start']>=sect_start) & (times['end']<=sect_end))]\n",
    "    # print('times after', times.shape)\n",
    "    time_types = dict(zip(times['id'], times['type']))\n",
    "    \n",
    "    nodes_keep = list(events['id']) + list(times['id'])\n",
    "    # print(len(nodes_keep))\n",
    "    \n",
    "    all_links = pd.DataFrame(data[doc_id]['ClinicalNarrativeTemporalAnnotation']['TAGS']['TLINK'])\n",
    "    all_links = all_links.loc[all_links['type']!='']\n",
    "\n",
    "    links = all_links.loc[(all_links['id'].str.lower().str.contains('sectime')==False)]\n",
    "    \n",
    "    # FILTER 2: Exclude sectime links not about admission\n",
    "    if section == 'history':\n",
    "        section_links = all_links.loc[(all_links['id'].str.lower().str.contains('sectime')==True) & (all_links['toID']=='T0')]\n",
    "    elif section == 'other':\n",
    "        section_links = all_links.loc[(all_links['id'].str.lower().str.contains('sectime')==True) & (all_links['toID']=='T1')]\n",
    "    else:\n",
    "        links = all_links\n",
    "    # print(section_links.shape)\n",
    "    # print(section_links.head())\n",
    "    # print(section_links.groupby('type')['fromID'].unique())\n",
    "    # print(set(section_links['fromID']) - set(nodes_keep))\n",
    "    # print(set(nodes_keep) -  set(section_links['fromID']))\n",
    "    if section != 'all':\n",
    "        node_category = dict(zip(section_links['fromID'], section_links['type']))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Normalize AFTER and BEFORE relations\n",
    "    links = links.copy()\n",
    "    mask = (links['type'] == 'AFTER')\n",
    "    links.loc[mask, ['fromID', 'fromText', 'toID', 'toText']] = links.loc[mask, ['toID', 'toText', 'fromID', 'fromText']].values\n",
    "    links.loc[mask, 'type'] = 'BEFORE'\n",
    "    links = links.drop_duplicates(subset=['fromID', 'fromText', 'toID', 'toText', 'type'], keep='last')\n",
    "    \n",
    "    \n",
    "    G = nx.from_pandas_edgelist(links[['fromID', 'toID', 'type']], source='fromID', target='toID', edge_attr=True, create_using=nx.DiGraph())\n",
    "    source_nodes = dict(zip(links['fromID'], links['fromText']))\n",
    "    target_nodes = dict(zip(links['toID'], links['toText']))\n",
    "    nx.set_node_attributes(G, source_nodes|target_nodes, 'text')\n",
    "    if section != 'all':\n",
    "        nx.set_node_attributes(G, node_category, 'time2section')\n",
    "    nx.set_node_attributes(G, event_types|time_types, 'type')\n",
    "    \n",
    "    # only keep nodes of interest\n",
    "    # FILTER 3: only subgraph\n",
    "    G = G.subgraph(nodes_keep).copy()\n",
    "    \n",
    "    # clear reverse links and reduce redundent nodes; \n",
    "    # There are no many duplicated links\n",
    "    return G, text[sect_start:sect_end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "a17798a3-14b1-42ac-bb25-b2261929447a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# G, text = build_section_graph('36.xml', train_data, 'all')\n",
    "# G, text = build_section_graph('36.xml', train_data, 'history')\n",
    "G, text = build_section_graph('36.xml', train_data, 'other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "7e9a079e-801b-45a8-aab9-736669887c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph with 134 nodes and 116 edges\n"
     ]
    }
   ],
   "source": [
    "print(f\"Graph with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "6ced4d13-6ad7-4c7c-ab21-cc41a2ad3a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "G, text = build_section_graph('36.xml', train_data, 'history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "fa43bcd9-0a1b-400c-88b9-646ece49a5c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph with 183 nodes and 177 edges\n"
     ]
    }
   ],
   "source": [
    "print(f\"Graph with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d7b4925-c93d-4675-bf74-e01d6d5b8838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph with 45 nodes and 42 edges\n"
     ]
    }
   ],
   "source": [
    "print(f\"Graph with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7bac7149-1cac-40e6-b5ed-595b1faa4c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_graphml(G, wp+\"graphs/tem_history_graph.graphml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bad22b-d17c-406f-87ca-4844dfa2500b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f3b70aa-49ae-4894-b96d-3947694c9f8d",
   "metadata": {},
   "source": [
    "### Merge overlap nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "4c5c0909-72d8-496a-8a9b-5a0b5597aa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_overlapping_nodes(G):\n",
    "    \"\"\"\n",
    "    Merge nodes that are connected by edges with type='overlp'.\n",
    "    Only merges edges across merged groups if they have the same direction.\n",
    "    Maintains node attributes and edge attributes as JSON strings for GraphML compatibility.\n",
    "    Tracks source nodes for each merged edge.\n",
    "    \n",
    "    Args:\n",
    "        G (nx.Graph or nx.DiGraph): Input graph\n",
    "        \n",
    "    Returns:\n",
    "        nx.Graph or nx.DiGraph: New graph with merged nodes and edges\n",
    "    \"\"\"\n",
    "    # Create new merged graph of same type as input\n",
    "    merged_G = G.__class__()\n",
    "    \n",
    "    # Find connected components considering only overlap edges\n",
    "    overlap_edges = [(u, v) for u, v, d in G.edges(data=True) if d.get('type') == 'OVERLAP']\n",
    "    overlap_graph = nx.Graph()  # Undirected for finding components\n",
    "    overlap_graph.add_edges_from(overlap_edges)\n",
    "    \n",
    "    # Get clusters of nodes to merge\n",
    "    clusters = list(nx.connected_components(overlap_graph))\n",
    "    \n",
    "    # Create mapping from original nodes to their merged cluster names\n",
    "    node_to_cluster = {}\n",
    "    for cluster in clusters:\n",
    "        cluster = list(cluster)\n",
    "        merged_name = '+'.join(sorted(cluster))\n",
    "        for node in cluster:\n",
    "            node_to_cluster[node] = merged_name\n",
    "    \n",
    "    # Process nodes\n",
    "    for cluster in clusters:\n",
    "        cluster = list(cluster)\n",
    "        \n",
    "        if len(cluster) == 1:\n",
    "            # Single node, just copy it and its attributes\n",
    "            node = cluster[0]\n",
    "            merged_G.add_node(node, **G.nodes[node])\n",
    "            continue\n",
    "            \n",
    "        # Create merged node name\n",
    "        merged_node = '+'.join(sorted(cluster))\n",
    "        \n",
    "        # Combine node attributes and convert to JSON string\n",
    "        merged_attrs = {\n",
    "            'original_nodes': json.dumps(cluster),\n",
    "            'node_attributes': json.dumps({node: dict(G.nodes[node]) for node in cluster})\n",
    "        }\n",
    "        \n",
    "        # Add merged node\n",
    "        merged_G.add_node(merged_node, **merged_attrs)\n",
    "    \n",
    "    # Add nodes that weren't in any cluster\n",
    "    unclustered_nodes = set(G.nodes()) - set(node for cluster in clusters for node in cluster)\n",
    "    for node in unclustered_nodes:\n",
    "        merged_G.add_node(node, **G.nodes[node])\n",
    "        node_to_cluster[node] = node  # Map to itself\n",
    "    \n",
    "    # Create a dictionary to store edges between clusters\n",
    "    cluster_edges = {}  # (from_cluster, to_cluster) -> list of original edges with source info\n",
    "    \n",
    "    # Process edges\n",
    "    for u, v, data in G.edges(data=True):\n",
    "        # Get cluster names (or original names for unclustered nodes)\n",
    "        u_cluster = node_to_cluster[u]\n",
    "        v_cluster = node_to_cluster[v]\n",
    "        \n",
    "        # Skip internal edges of merged clusters if they were overlap edges\n",
    "        if u_cluster == v_cluster and data.get('type') == 'OVERLAP':\n",
    "            continue\n",
    "        \n",
    "        # Add source node information to edge data\n",
    "        edge_data = data.copy()\n",
    "        edge_data['source_nodes'] = {'from': u, 'to': v}\n",
    "        \n",
    "        # Create edge key based on direction\n",
    "        edge_key = (u_cluster, v_cluster)\n",
    "        \n",
    "        # For directed graphs, maintain direction information\n",
    "        if isinstance(G, nx.DiGraph):\n",
    "            if edge_key not in cluster_edges:\n",
    "                cluster_edges[edge_key] = []\n",
    "            cluster_edges[edge_key].append((u, v, edge_data))\n",
    "        else:\n",
    "            # For undirected graphs, normalize the edge key\n",
    "            normalized_key = tuple(sorted([u_cluster, v_cluster]))\n",
    "            if normalized_key not in cluster_edges:\n",
    "                cluster_edges[normalized_key] = []\n",
    "            cluster_edges[normalized_key].append((u, v, edge_data))\n",
    "    \n",
    "    # Add merged edges to the graph\n",
    "    for (from_cluster, to_cluster), edges in cluster_edges.items():\n",
    "        # For directed graphs, check if all edges have the same direction\n",
    "        if isinstance(G, nx.DiGraph):\n",
    "            # Check if all edges go in the same direction\n",
    "            directions = set((u_cluster, v_cluster) \n",
    "                           for u, v, _ in edges\n",
    "                           for u_cluster, v_cluster in [(node_to_cluster[u], node_to_cluster[v])])\n",
    "            \n",
    "            if len(directions) == 1:  # All edges have same direction\n",
    "                merged_G.add_edge(from_cluster, to_cluster, \n",
    "                                edge_attributes=json.dumps([data for _, _, data in edges]))\n",
    "        else:\n",
    "            # For undirected graphs, just add the edge\n",
    "            merged_G.add_edge(from_cluster, to_cluster, \n",
    "                            edge_attributes=json.dumps([data for _, _, data in edges]))\n",
    "    \n",
    "    return merged_G\n",
    "\n",
    "def load_merged_graph(graphml_file):\n",
    "    \"\"\"\n",
    "    Load a merged graph from GraphML file and convert JSON string attributes back to Python objects.\n",
    "    \"\"\"\n",
    "    G = nx.read_graphml(graphml_file)\n",
    "    \n",
    "    # Convert node attributes back from JSON\n",
    "    for node in G.nodes():\n",
    "        if 'original_nodes' in G.nodes[node]:\n",
    "            G.nodes[node]['original_nodes'] = json.loads(G.nodes[node]['original_nodes'])\n",
    "            G.nodes[node]['node_attributes'] = json.loads(G.nodes[node]['node_attributes'])\n",
    "    \n",
    "    # Convert edge attributes back from JSON\n",
    "    for u, v in G.edges():\n",
    "        if 'edge_attributes' in G[u][v]:\n",
    "            G[u][v]['edge_attributes'] = json.loads(G[u][v]['edge_attributes'])\n",
    "    \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "68014572-7ea3-48de-9a7a-bec02d2c9cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mergaed_G = merge_overlapping_nodes(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "4e20d87b-48e0-49bf-ab5f-b7d66b66d521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph with 70 nodes and 51 edges\n"
     ]
    }
   ],
   "source": [
    "print(f\"Graph with {mergaed_G.number_of_nodes()} nodes and {mergaed_G.number_of_edges()} edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ceb67fa0-b009-4592-92d8-d5f927263339",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_graphml(mergaed_G, wp+\"graphs/tem_history_graph_merge.graphml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0a9f53a9-a5cb-4643-9257-3de36fc85d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('E102+E105+E106+E111+E112+E113+E114+E115+E4+E5+E6+E7+E8+T4+T6+T7', {'original_nodes': '[\"E102\", \"E113\", \"E115\", \"E112\", \"E5\", \"T4\", \"E8\", \"E7\", \"E111\", \"E4\", \"E105\", \"T7\", \"E6\", \"E114\", \"E106\", \"T6\"]', 'node_attributes': '{\"E102\": {\"text\": \"short of breath\", \"time2section\": \"BEFORE\", \"type\": \"PROBLEM\"}, \"E113\": {\"text\": \"lower extremity edema\", \"time2section\": \"BEFORE\", \"type\": \"PROBLEM\"}, \"E115\": {\"text\": \"Her lower extremity edema\", \"time2section\": \"BEFORE\", \"type\": \"PROBLEM\"}, \"E112\": {\"text\": \"sharp\", \"time2section\": \"BEFORE\", \"type\": \"PROBLEM\"}, \"E5\": {\"text\": \"sweating\", \"time2section\": \"BEFORE\", \"type\": \"PROBLEM\"}, \"T4\": {\"text\": \"several years\", \"type\": \"DURATION\"}, \"E8\": {\"text\": \"syncope\", \"time2section\": \"BEFORE\", \"type\": \"PROBLEM\"}, \"E7\": {\"text\": \"vomiting\", \"time2section\": \"BEFORE\", \"type\": \"PROBLEM\"}, \"E111\": {\"text\": \"nonradiating\", \"time2section\": \"BEFORE\", \"type\": \"PROBLEM\"}, \"E4\": {\"text\": \"chest twinges\", \"time2section\": \"BEFORE\", \"type\": \"PROBLEM\"}, \"E105\": {\"text\": \"Her shortness of breath\", \"time2section\": \"BEFORE\", \"type\": \"PROBLEM\"}, \"T7\": {\"text\": \"several years\", \"type\": \"DURATION\"}, \"E6\": {\"text\": \"nausea\", \"time2section\": \"BEFORE\", \"type\": \"PROBLEM\"}, \"E114\": {\"text\": \"cellulitis\", \"time2section\": \"BEFORE\", \"type\": \"PROBLEM\"}, \"E106\": {\"text\": \"dyspnea\", \"time2section\": \"BEFORE\", \"type\": \"PROBLEM\"}, \"T6\": {\"text\": \"a few seconds\", \"type\": \"DURATION\"}}'}), ('E1+E107+E109+E110+E2+E3+E83+E95+E96+E97+E98+T2+T3+T5', {'original_nodes': '[\"E2\", \"E3\", \"T3\", \"E97\", \"E110\", \"E83\", \"E95\", \"E109\", \"E1\", \"E107\", \"E96\", \"T2\", \"E98\", \"T5\"]', 'node_attributes': '{\"E2\": {\"text\": \"chills\", \"time2section\": \"BEFORE\", \"type\": \"PROBLEM\"}, \"E3\": {\"text\": \"leg pain\", \"time2section\": \"BEFORE\", \"type\": \"PROBLEM\"}, \"T3\": {\"text\": \"2-3 years\", \"type\": \"DURATION\"}, \"E97\": {\"text\": \"an associated dry cough\", \"time2section\": \"BEFORE\", \"type\": \"PROBLEM\"}, \"E110\": {\"text\": \"noparoxysmal nocturnal dyspnea\", \"time2section\": \"BEFORE\", \"type\": \"PROBLEM\"}, \"E83\": {\"text\": \"presents\", \"time2section\": \"BEFORE\", \"type\": \"OCCURRENCE\"}, \"E95\": {\"text\": \"increased shortness of breath\", \"time2section\": \"BEFORE\", \"type\": \"PROBLEM\"}, \"E109\": {\"text\": \"orthopnea\", \"time2section\": \"BEFORE\", \"type\": \"PROBLEM\"}, \"E1\": {\"text\": \"fevers\", \"time2section\": \"BEFORE\", \"type\": \"PROBLEM\"}, \"E107\": {\"text\": \"sleeps in a chair up right\", \"time2section\": \"BEFORE\", \"type\": \"OCCURRENCE\"}, \"E96\": {\"text\": \"Her shortness of breath\", \"time2section\": \"BEFORE\", \"type\": \"PROBLEM\"}, \"T2\": {\"text\": \"5 days\", \"type\": \"DURATION\"}, \"E98\": {\"text\": \"dyspnea\", \"time2section\": \"BEFORE\", \"type\": \"PROBLEM\"}, \"T5\": {\"text\": \"2 1/2 years\", \"type\": \"DURATION\"}}'}), ('E93+E94', {'original_nodes': '[\"E93\", \"E94\"]', 'node_attributes': '{\"E93\": {\"text\": \"obesity\", \"time2section\": \"BEFORE\", \"type\": \"PROBLEM\"}, \"E94\": {\"text\": \"hypertension\", \"time2section\": \"BEFORE\", \"type\": \"PROBLEM\"}}'}), ('E100+E82+E99', {'original_nodes': '[\"E99\", \"E100\", \"E82\"]', 'node_attributes': '{\"E99\": {\"text\": \"walker\", \"time2section\": \"BEFORE\", \"type\": \"TREATMENT\"}, \"E100\": {\"text\": \"a cane\", \"time2section\": \"BEFORE\", \"type\": \"TREATMENT\"}, \"E82\": {\"text\": \"ambulates\", \"time2section\": \"BEFORE\", \"type\": \"OCCURRENCE\"}}'}), ('E103+E104', {'original_nodes': '[\"E103\", \"E104\"]', 'node_attributes': '{\"E103\": {\"text\": \"getting up from her chair\", \"time2section\": \"BEFORE\", \"type\": \"OCCURRENCE\"}, \"E104\": {\"text\": \"light headed\", \"time2section\": \"BEFORE\", \"type\": \"PROBLEM\"}}'}), ('E9', {'text': 'any pleural chest pain', 'time2section': 'BEFORE', 'type': 'PROBLEM'}), ('T8', {'text': 'the several weeks', 'type': 'DURATION'}), ('E118', {'text': 'a broken chair', 'time2section': 'BEFORE', 'type': 'OCCURRENCE'}), ('E101', {'text': 'osteoarthritis', 'time2section': 'BEFORE', 'type': 'PROBLEM'}), ('E119', {'text': 'denies', 'time2section': 'AFTER', 'type': 'EVIDENTIAL'}), ('E116', {'text': 'admission', 'time2section': 'OVERLAP', 'type': 'OCCURRENCE'}), ('E117', {'text': 'an inability to elevate her legs', 'time2section': 'BEFORE', 'type': 'OCCURRENCE'}), ('E108', {'text': 'osteoarthritis', 'time2section': 'BEFORE', 'type': 'PROBLEM'})]\n"
     ]
    }
   ],
   "source": [
    "print(list(mergaed_G.nodes(data=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02b554d-ba69-413d-a6cf-9755aaa5c841",
   "metadata": {},
   "source": [
    "### Remove conflicting relations (e.g., self-link and mutual links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "a799f672-1938-4398-9e6f-017630da4983",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_self_links(G):\n",
    "    H = G.copy()\n",
    "    self_loops = list(nx.selfloop_edges(H))\n",
    "    H.remove_edges_from(self_loops)\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "37831800-2134-45b6-8aac-5b67b35d2f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_mutual_links(G):\n",
    "    H = G.copy()\n",
    "    edges_to_remove = set()\n",
    "    \n",
    "    for u, v in G.edges():\n",
    "        if H.has_edge(v, u) and (v, u) not in edges_to_remove and (u, v) not in edges_to_remove:\n",
    "            edge1 = G.get_edge_data(u, v)\n",
    "            edge2 = G.get_edge_data(v, u)\n",
    "            edges_to_remove.add((u, v))\n",
    "            edges_to_remove.add((v, u))\n",
    "    H.remove_edges_from(edges_to_remove)\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "c393bd1b-f14b-476b-a1d2-1a37f6525f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mergaed_G_clean = remove_self_links(mergaed_G)\n",
    "mergaed_G_clean = remove_mutual_links(mergaed_G_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "59f526db-d5a7-4d8c-b172-ed057a9ea8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph with 70 nodes and 51 edges\n"
     ]
    }
   ],
   "source": [
    "print(f\"Graph with {mergaed_G_clean.number_of_nodes()} nodes and {mergaed_G_clean.number_of_edges()} edges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f653d8-94fc-44b6-83ea-07b65e5b951e",
   "metadata": {},
   "source": [
    "### Remove redundant links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "c1384a7b-ae17-4dd6-97aa-9a0ef1eebb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_redundant_edges(G):\n",
    "    redundant_edges = []\n",
    "    \n",
    "    # Iterate over edges while capturing their attributes\n",
    "    edges = list(G.edges(data=True))  # List of tuples (u, v, data_dict)\n",
    "    \n",
    "    for u, v, data in edges:\n",
    "        # Remove the edge and check if a path still exists\n",
    "        G.remove_edge(u, v)\n",
    "        \n",
    "        if nx.has_path(G, u, v):\n",
    "            redundant_edges.append((u, v))\n",
    "        \n",
    "        # Re-add the edge with its original attributes\n",
    "        G.add_edge(u, v, **data)\n",
    "    \n",
    "    # Remove redundant edges (preserves attributes of non-redundant edges)\n",
    "    G.remove_edges_from(redundant_edges)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "d6519493-50cd-4d39-9b13-a880c9c53ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mergaed_G_clean = remove_redundant_edges(mergaed_G_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "9806404d-6529-4659-851d-ce5f1dea5e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph with 70 nodes and 51 edges\n"
     ]
    }
   ],
   "source": [
    "print(f\"Graph with {mergaed_G_clean.number_of_nodes()} nodes and {mergaed_G_clean.number_of_edges()} edges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd70f989-585a-47cd-b7df-2714eb22844d",
   "metadata": {},
   "source": [
    "### Minimal paths and clean nodes rather than 'PROBLEM', 'TEST', 'TREATMENT'. A --> B --> C  and D-->C "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "ae72d5cf-46bf-4953-bc20-fbe015a5276e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimal_path_cover(G):\n",
    "    \"\"\"\n",
    "    Find a minimal collection of paths that cover all edges in a directed graph.\n",
    "    \n",
    "    Args:\n",
    "        G: A NetworkX directed graph (DiGraph)\n",
    "        \n",
    "    Returns:\n",
    "        A list of paths, where each path is a list of nodes\n",
    "    \"\"\"\n",
    "    if not G.edges():\n",
    "        return []\n",
    "    \n",
    "    # Create a working copy of the graph\n",
    "    remaining_edges = G.copy()\n",
    "    paths = []\n",
    "    \n",
    "    while remaining_edges.edges():\n",
    "        # Find longest path in the remaining graph\n",
    "        # This is a greedy approach - finding the truly minimal cover is NP-hard\n",
    "        longest_path = find_longest_path(remaining_edges)\n",
    "        \n",
    "        # Add the path to our collection\n",
    "        paths.append(longest_path)\n",
    "        \n",
    "        # Remove the edges in this path from the remaining graph\n",
    "        for i in range(len(longest_path) - 1):\n",
    "            u, v = longest_path[i], longest_path[i + 1]\n",
    "            if remaining_edges.has_edge(u, v):\n",
    "                remaining_edges.remove_edge(u, v)\n",
    "    \n",
    "    return paths\n",
    "\n",
    "def find_longest_path(G):\n",
    "    \"\"\"\n",
    "    Find the longest path in a directed graph.\n",
    "    \n",
    "    Args:\n",
    "        G: A NetworkX directed graph (DiGraph)\n",
    "        \n",
    "    Returns:\n",
    "        A list of nodes representing the longest path\n",
    "    \"\"\"\n",
    "    # For each node, try to find the longest path starting from it\n",
    "    longest_path = []\n",
    "    \n",
    "    for start_node in G.nodes():\n",
    "        # Skip nodes with no outgoing edges\n",
    "        if G.out_degree(start_node) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Find the longest path from this start node\n",
    "        path = find_longest_path_from_node(G, start_node)\n",
    "        \n",
    "        # Update longest path if this one is longer\n",
    "        if len(path) > len(longest_path):\n",
    "            longest_path = path\n",
    "    \n",
    "    return longest_path\n",
    "\n",
    "def find_longest_path_from_node(G, start_node):\n",
    "    \"\"\"\n",
    "    Find the longest path starting from a specific node.\n",
    "    \n",
    "    Args:\n",
    "        G: A NetworkX directed graph (DiGraph)\n",
    "        start_node: The starting node\n",
    "        \n",
    "    Returns:\n",
    "        A list of nodes representing the longest path from start_node\n",
    "    \"\"\"\n",
    "    # Use dynamic programming to find the longest path\n",
    "    # This is much more efficient than a brute force approach\n",
    "    \n",
    "    # Initialize distances and paths\n",
    "    dist = {node: -float('inf') for node in G.nodes()}\n",
    "    dist[start_node] = 0\n",
    "    pred = {node: None for node in G.nodes()}\n",
    "    \n",
    "    # Topologically sort the nodes\n",
    "    try:\n",
    "        topo_order = list(nx.topological_sort(G))\n",
    "    except nx.NetworkXUnfeasible:\n",
    "        # Graph has cycles, so we'll use a heuristic approach\n",
    "        # For simplicity, we'll use a DFS-based approach\n",
    "        visited = set()\n",
    "        path = [start_node]\n",
    "        current_path = []\n",
    "        dfs_longest_path(G, start_node, visited, path, current_path)\n",
    "        return current_path\n",
    "    \n",
    "    # Dynamic programming to find longest path\n",
    "    for node in topo_order:\n",
    "        for successor in G.successors(node):\n",
    "            if dist[successor] < dist[node] + 1:\n",
    "                dist[successor] = dist[node] + 1\n",
    "                pred[successor] = node\n",
    "    \n",
    "    # Find the node with the maximum distance\n",
    "    end_node = max(dist, key=dist.get)\n",
    "    \n",
    "    # Reconstruct the path\n",
    "    path = []\n",
    "    while end_node is not None:\n",
    "        path.append(end_node)\n",
    "        end_node = pred[end_node]\n",
    "    \n",
    "    # Reverse to get from start to end\n",
    "    return path[::-1]\n",
    "\n",
    "def dfs_longest_path(G, node, visited, path, longest_path):\n",
    "    \"\"\"\n",
    "    DFS helper for finding the longest path in a graph with cycles.\n",
    "    \n",
    "    Args:\n",
    "        G: A NetworkX directed graph (DiGraph)\n",
    "        node: Current node\n",
    "        visited: Set of visited nodes in current path\n",
    "        path: Current path\n",
    "        longest_path: Reference to the longest path found so far\n",
    "    \"\"\"\n",
    "    visited.add(node)\n",
    "    \n",
    "    for neighbor in G.successors(node):\n",
    "        if neighbor not in visited:\n",
    "            path.append(neighbor)\n",
    "            dfs_longest_path(G, neighbor, visited, path, longest_path)\n",
    "            path.pop()\n",
    "    \n",
    "    if len(path) > len(longest_path):\n",
    "        longest_path.clear()\n",
    "        longest_path.extend(path)\n",
    "    \n",
    "    visited.remove(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "76166615-b33c-4791-a089-a3fd3a673024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type_attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "7e5882f7-a249-49b5-8359-1014aaa25b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_graphml(mergaed_G, wp+\"graphs/tem_other_graph_merge.graphml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "7c032c2e-34f6-46a0-9723-adae7b8cf211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimal path cover:\n",
      "Path 1: ['E68', 'E54+E67', 'E66', 'E64+E65+E71', 'E69', 'E70+T16', 'E160']\n",
      "BEFORE\n",
      "BEFORE ....\n",
      "BEFORE ....\n",
      "BEFORE\n",
      "BEFORE ....\n",
      "BEFORE ....\n",
      "BEFORE ....\n",
      "BEFORE\n",
      "BEFORE ....\n",
      "None ....\n",
      "poor perfusion ---> edema, her edema ---> distention ---> Infectious disease, her cellulitis, cellulitis on her legs ---> cefazolin ---> Keflex, 10 more days\n",
      "Path 2: ['E142+E47+E52+T13', 'E141+E146', 'E145+E50+E51', 'E49', 'E89']\n",
      "None ....\n",
      "BEFORE ....\n",
      "BEFORE ....\n",
      "BEFORE ....\n",
      "BEFORE ....\n",
      "BEFORE ....\n",
      "AFTER ....\n",
      "BEFORE\n",
      "q day, hydrochlorothiazide, Lisinopril, an aspirin --->  ---> heart rate, gentle doses, careful monitoring ---> bradycardia\n",
      "Path 3: ['E123', 'E122', 'E124+E21', 'E121+E129+E84', 'E125+E126']\n",
      "BEFORE\n",
      "BEFORE\n",
      "BEFORE ....\n",
      "BEFORE ....\n",
      "BEFORE ....\n",
      "hypertension ---> diastolic dysfunction ---> congestive heart failure, pulmonary hypertension --->  ---> negative enzymes\n",
      "Path 4: ['E135+E136', 'E131+E132+E133+E134+E34+E35+E36+E37+E39', 'E41']\n",
      "BEFORE ....\n",
      "BEFORE ....\n",
      "BEFORE ....\n",
      "BEFORE ....\n",
      "BEFORE ....\n",
      "BEFORE ....\n",
      "BEFORE ....\n",
      "BEFORE ....\n",
      "BEFORE ....\n",
      "BEFORE ....\n",
      "BEFORE\n",
      "an intra-atrial shunt, a patent foramen ovale ---> left atrial enlargement, 44 plus right atrial pressure, a poor study, mild mitral regurgitation, pulmonary artery pressures, A TTE, an ejection fraction, mild tricuspid regurgitation ---> subsequent dobutamine MIBI\n",
      "Path 5: ['E44+E45', 'E46', 'E140+T12']\n",
      "BEFORE ....\n",
      "BEFORE ....\n",
      "BEFORE\n",
      "None ....\n",
      "Her blood pressure, high ---> Captopril ---> t.i.d.\n",
      "Path 6: ['E57+E73+E74', 'E31+E72+E90', 'E32']\n",
      "BEFORE ....\n",
      "BEFORE ....\n",
      "BEFORE ....\n",
      "BEFORE ....\n",
      "BEFORE ....\n",
      "BEFORE\n",
      "deep venous thrombosis, a right common femoral clot, This clot ---> a D-dimer, D-dimer ---> lower extremity non-invasive studies\n",
      "Path 7: ['E155+E156+E161+E80', 'E162', 'E91+T18']\n",
      "BEFORE ....\n",
      "BEFORE\n",
      "None ....\n",
      "Heparin ---> Lovenox ---> q day\n",
      "Path 8: ['E53', 'E141+E146', 'E154']\n",
      "BEFORE\n",
      "AFTER\n",
      "Lasix --->  ---> pulmonary function studies\n",
      "Path 9: ['E20', 'E121+E129+E84', 'E120']\n",
      "BEFORE\n",
      "acute and chronic shortness of breath ---> \n",
      "Path 10: ['E85', 'E127+E23', 'E128+T9']\n",
      "BEFORE ....\n",
      "BEFORE ....\n",
      "None ....\n",
      "diuresed, Lasix ---> several days\n",
      "Path 11: ['E42+E43', 'E41']\n",
      "BEFORE ....\n",
      "BEFORE ....\n",
      "BEFORE\n",
      "a small to medium perfusion defect, inducible ischemia ---> subsequent dobutamine MIBI\n",
      "Path 12: ['E142+E47+E52+T13', 'E143']\n",
      "None ....\n",
      "BEFORE ....\n",
      "BEFORE ....\n",
      "BEFORE ....\n",
      "q day, hydrochlorothiazide, Lisinopril, an aspirin\n",
      "Path 13: ['E57+E73+E74', 'E148']\n",
      "BEFORE ....\n",
      "BEFORE ....\n",
      "BEFORE ....\n",
      "BEFORE\n",
      "deep venous thrombosis, a right common femoral clot, This clot ---> Coumadin\n",
      "Path 14: ['E150+E152', 'E153+E63+T15']\n",
      "BEFORE ....\n",
      "BEFORE ....\n",
      "None ....\n",
      "BEFORE ....\n",
      "severe sleep apnea, not breathing ---> 3 nights, BIPAP\n",
      "Path 15: ['E150+E152', 'E151']\n",
      "BEFORE ....\n",
      "BEFORE ....\n",
      "BEFORE\n",
      "severe sleep apnea, not breathing ---> falls asleep midsentence\n",
      "Path 16: ['E155+E156+E161+E80', 'E154']\n",
      "BEFORE ....\n",
      "AFTER\n",
      "Heparin ---> pulmonary function studies\n",
      "Path 17: ['E155+E156+E161+E80', 'E157']\n",
      "BEFORE ....\n",
      "BEFORE\n",
      "Heparin ---> the above testing\n",
      "Path 18: ['E155+E156+E161+E80', 'E158']\n",
      "BEFORE ....\n",
      "BEFORE\n",
      "Heparin ---> home oxygen\n",
      "Path 19: ['E166+T19', 'E165']\n",
      "BEFORE ....\n",
      "None ....\n",
      "Lovenox, 1 day\n",
      "Path 20: ['E164+E81', 'E167']\n",
      "BEFORE ....\n",
      "AFTER\n",
      "Discharge INR ---> Goal INR\n",
      "Path 21: ['E168+E169', 'E165']\n",
      "AFTER ....\n",
      "AFTER ....\n",
      "Low cholesterol , low fat diet, Low salt diet\n",
      "Path 22: ['E170', 'E75']\n",
      "BEFORE\n",
      "anticoagulation\n",
      "Path 23: ['E76', 'E75']\n",
      "BEFORE\n",
      "BEFORE\n",
      "the positive D-dimer ---> anticoagulation\n",
      "Path 24: ['T17', 'E91+T18']\n",
      "None\n",
      "None ....\n",
      "the day prior to discharge ---> q day\n",
      "Path 25: ['E159', 'E158']\n",
      "BEFORE\n",
      "BEFORE\n",
      "2 liters nasal cannula oxygen ---> home oxygen\n",
      "Path 26: ['E25', 'E24+E87+T10']\n",
      "BEFORE\n",
      "BEFORE ....\n",
      "None ....\n",
      "her bicarbonate ---> The diuresis, q day\n",
      "Path 27: ['E48', 'E49']\n",
      "BEFORE\n",
      "BEFORE\n",
      "Her atenolol ---> bradycardia\n",
      "Path 28: ['E137', 'E138+E139']\n",
      "BEFORE ....\n",
      "BEFORE ....\n",
      "catheterization, medical management\n",
      "Path 29: ['E147', 'E55']\n",
      "BEFORE\n",
      "BEFORE\n",
      "Pulmonary hypertension ---> echocardiogram\n",
      "Path 30: ['E22', 'E125+E126']\n",
      "BEFORE\n",
      "BEFORE ....\n",
      "myocardial infarction ---> negative enzymes\n",
      "Path 31: ['E33', 'E32']\n",
      "BEFORE\n",
      "BEFORE\n",
      "a small right common femoral clot ---> lower extremity non-invasive studies\n",
      "Path 32: ['E86', 'E24+E87+T10']\n",
      "BEFORE ....\n",
      "None ....\n",
      "The diuresis, q day\n",
      "Path 33: ['E86', 'E130']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "covering_paths = minimal_path_cover(mergaed_G_clean)\n",
    "G = mergaed_G_clean.copy()\n",
    "print(\"Minimal path cover:\")\n",
    "for i, path in enumerate(covering_paths):\n",
    "    print(f\"Path {i+1}: {path}\")\n",
    "    texts = []\n",
    "    for node in path:\n",
    "        \n",
    "        text = G.nodes[node].get(\"text\", None)\n",
    "        type = G.nodes[node].get(\"type\", None)\n",
    "        time2section = G.nodes[node].get(\"time2section\", None)\n",
    "        if type == None:\n",
    "            tlist = []\n",
    "            type_attrs = G.nodes[node].get(\"node_attributes\")\n",
    "            type_attrs = json.loads(type_attrs)\n",
    "            for nid in type_attrs.keys():\n",
    "                # print(type_attrs[nid])\n",
    "                text = type_attrs[nid]['text']\n",
    "                type = type_attrs[nid]['type']\n",
    "                \n",
    "                time2section = type_attrs[nid].get('time2section', None)\n",
    "                if type in ['PROBLEM', 'TEST', 'TREATMENT', 'DURATION', 'DATE', 'FREQUENCY']:\n",
    "                    tlist.append(text)\n",
    "                    print(time2section, '....')\n",
    "            texts.append(', '.join(tlist))\n",
    "        else:\n",
    "            if type in ['PROBLEM', 'TEST', 'TREATMENT', 'DURATION', 'DATE', 'FREQUENCY']:\n",
    "                texts.append(text)\n",
    "                print(time2section)\n",
    "    # print(texts)\n",
    "    print(' ---> '.join(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "32d4b36c-6b6a-454d-a386-6941abd4cc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO summarize path based on context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e7ce362c-1db6-4fb7-b8b4-a0d179874678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All paths in the graph with labels:\n",
      "['E102+E105+E106+E111+E112+E113+E114+E115+E4+E5+E6+E7+E8+T4+T6+T7', 'T8']\n",
      "\n",
      "['E102+E105+E106+E111+E112+E113+E114+E115+E4+E5+E6+E7+E8+T4+T6+T7', 'T8', 'E116']\n",
      "\n",
      "['E103+E104', 'E102+E105+E106+E111+E112+E113+E114+E115+E4+E5+E6+E7+E8+T4+T6+T7']\n",
      "\n",
      "['E103+E104', 'E102+E105+E106+E111+E112+E113+E114+E115+E4+E5+E6+E7+E8+T4+T6+T7', 'T8']\n",
      "\n",
      "['E103+E104', 'E102+E105+E106+E111+E112+E113+E114+E115+E4+E5+E6+E7+E8+T4+T6+T7', 'T8', 'E116']\n",
      "\n",
      "['E9', 'E119']\n",
      "\n",
      "['T8', 'E116']\n",
      "\n",
      "['E118', 'E117', 'E102+E105+E106+E111+E112+E113+E114+E115+E4+E5+E6+E7+E8+T4+T6+T7']\n",
      "\n",
      "['E118', 'E117', 'E102+E105+E106+E111+E112+E113+E114+E115+E4+E5+E6+E7+E8+T4+T6+T7', 'T8']\n",
      "\n",
      "['E118', 'E117', 'E102+E105+E106+E111+E112+E113+E114+E115+E4+E5+E6+E7+E8+T4+T6+T7', 'T8', 'E116']\n",
      "\n",
      "['E118', 'E117']\n",
      "\n",
      "['E101', 'E100+E82+E99']\n",
      "\n",
      "['E117', 'E102+E105+E106+E111+E112+E113+E114+E115+E4+E5+E6+E7+E8+T4+T6+T7']\n",
      "\n",
      "['E117', 'E102+E105+E106+E111+E112+E113+E114+E115+E4+E5+E6+E7+E8+T4+T6+T7', 'T8']\n",
      "\n",
      "['E117', 'E102+E105+E106+E111+E112+E113+E114+E115+E4+E5+E6+E7+E8+T4+T6+T7', 'T8', 'E116']\n",
      "\n",
      "['E108', 'E1+E107+E109+E110+E2+E3+E83+E95+E96+E97+E98+T2+T3+T5']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "G = mergaed_G_clean.copy()\n",
    "print(\"All paths in the graph with labels:\")\n",
    "for source in G.nodes:\n",
    "    for target in G.nodes:\n",
    "        if source != target:\n",
    "            # Get all simple paths\n",
    "            paths = list(nx.all_simple_paths(G, source=source, target=target))\n",
    "            for path in paths:\n",
    "                # Convert node IDs to their labels\n",
    "                print(path)\n",
    "                # nodes_types = [G.nodes[node][\"type\"] for node in path]\n",
    "                # node_labels = [G.nodes[node][\"text\"] for node in path]\n",
    "\n",
    "                # # Find edges along the path and get their labels\n",
    "                # edge_labels = [\n",
    "                #     G.edges[path[i], path[i + 1]][\"type\"]\n",
    "                #     for i in range(len(path) - 1)\n",
    "                # ]\n",
    "\n",
    "                \n",
    "                # # Print the path with labels\n",
    "                # if len(set(['PROBLEM', 'TEST', 'TREATMENT']).intersection(set(nodes_types)))>0:\n",
    "                #     print(f\"Path: {' -> '.join(node_labels)}\")\n",
    "                #     print(f\"Path: {' -> '.join(nodes_types)}\")\n",
    "                #     print(f\"Edges: {' -> '.join(edge_labels)}\")\n",
    "                print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "eabebdec-c386-4cc6-b2ce-0eb3b8195a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path 1: ['E118', 'E117', 'E102+E105+E106+E111+E112+E113+E114+E115+E4+E5+E6+E7+E8+T4+T6+T7', 'T8', 'E116']\n",
    "# Path 2: ['E103+E104', 'E102+E105+E106+E111+E112+E113+E114+E115+E4+E5+E6+E7+E8+T4+T6+T7']\n",
    "# Path 3: ['E9', 'E119']\n",
    "# Path 4: ['E101', 'E100+E82+E99']\n",
    "# Path 5: ['E108', 'E1+E107+E109+E110+E2+E3+E83+E95+E96+E97+E98+T2+T3+T5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1f36f08c-b34b-4547-adc2-2dea047c293a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from lmformatenforcer import JsonSchemaParser\n",
    "from pydantic import BaseModel\n",
    "import re, json, os\n",
    "from typing import Optional, Type, TypeVar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ef6ba83d-2342-4812-89d1-2fb2fb44a6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify OpenAI's API key and API base to use vLLM's API server.\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://host.docker.internal:8000/v1\"\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "20f2df9b-1d0f-4689-be34-c48292276c2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Extract clinical 'PROBLEM', 'TEST', 'TREATMENT' from the text and estimate the time of each event happened.HISTORY OF PRESENT ILLNESS :\\nSaujule Study is a 77-year-old woman with a history of obesity and hypertension who presents with increased shortness of breath x 5 days .\\nHer shortness of breath has been progressive over the last 2-3 years .\\nShe has an associated dry cough but no fevers , chills , or leg pain .\\nShe has dyspnea on exertion .\\nShe ambulates with walker and a cane secondary to osteoarthritis .\\nShe becomes short of breath just by getting up from her chair and can only walk 2-3 steps on a flat surface .\\nShe feels light headed when getting up .\\nHer shortness of breath and dyspnea on exertion has been progressive for the past several years .\\nIt has not been sudden or acute .\\nShe sleeps in a chair up right for the last 2 1/2 years secondary to osteoarthritis .\\nShe has orthopnea as well but noparoxysmal nocturnal dyspnea .\\nShe occasionally feels chest twinges which are nonradiating but are sharp .\\nThey last a few seconds on the left side and are not associated with sweating , nausea , vomiting or syncope .\\nShe has had lower extremity edema for thelast several years with multiple episodes of cellulitis .\\nHer lower extremity edema has increased for the several weeks prior to admission secondary to an inability to elevate her legs due to a broken chair at home .\\nShe denies any pleural chest pain .\\n\""
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Extract clinical 'PROBLEM', 'TEST', 'TREATMENT' from the text and estimate the time of each event happened.\" + text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ea3e6dec-19b5-45f4-94d9-0128065d4fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat response: Alright, I need to figure out how to extract 'PROBLEM', 'TEST', and 'TREATMENT' from the provided patient history. The user also wants the time each event occurred estimated.\n",
      "\n",
      "First, looking for PROBLEMS. The patient is a 77-year-old woman with obesity and hypertension. The main presenting issue is increased shortness of breath for 5 days, but it's been progressing for 2-3 years. Also, she has a dry cough, orthopnea, dyspnea on exertion, and lower extremity edema with cellulitis episodes. She feels light-headed upon standing, has chest twinges, and sleeps upright.\n",
      "\n",
      "So, the PROBLEMS would include her chronic conditions like obesity, hypertension, and the various symptoms like SOB, dyspnea, orthopnea, edema, chest twinges, etc. I should list each clearly.\n",
      "\n",
      "Next, looking for TESTS. The text doesn't mention any specific tests she's had. It talks about her symptoms and management but not about any diagnostic procedures or lab tests. So, I'll note that no tests are mentioned in the provided text.\n",
      "\n",
      "For TREATMENT, she uses a walker and cane due to osteoarthritis. She sleeps upright because of that condition too. But there's no mention of medications or other therapies. So, the treatment is limited to mobility aids.\n",
      "\n",
      "Now, estimating when each event happened. Her increased SOB started 5 days ago. The progressive dyspnea and SOB were over 2-3 years. Orthopnea and sleeping upright started 2.5 years ago. Lower extremity edema and cellulitis episodes have been ongoing for several years. The inability to elevate legs due to a broken chair is recent, within several weeks prior to admission.\n",
      "\n",
      "I think that covers all the points. I'll structure this as bullet points for clarity, separating each category and time frame.\n",
      "</think>\n",
      "\n",
      "### Extracted Clinical Information:\n",
      "\n",
      "**PROBLEMS:**\n",
      "1. **Increased shortness of breath**: x5 days (recent onset)\n",
      "2. **Progressive shortness of breath**: Over the last 2-3 years (chronic)\n",
      "3. **Dry cough**: Currently present\n",
      "4. **Dyspnea on exertion**: Progressive over several years\n",
      "5. **Osteoarthritis**: Likely contributing to ambulation issues and orthopnea\n",
      "6. **Orthopnea**: Present for the last 2.5 years\n",
      "7. **Lower extremity edema**: With multiple episodes of cellulitis\n",
      "8. **Light-headedness upon standing**: Recent symptom\n",
      "9. **Chest twinges**: Occasional, nonradiating, sharp, lasting a few seconds on the left\n",
      "10. **Inability to elevate legs**: Due to a broken chair for several weeks prior to admission\n",
      "\n",
      "**TESTS:**\n",
      "No specific tests are mentioned in the provided text.\n",
      "\n",
      "**TREATMENTS:**\n",
      "1. Use of walker and cane for ambulation\n",
      "2. Sleeping upright in a chair for the last 2.5 years\n",
      "\n",
      "### Estimated Time of Each Event:\n",
      "\n",
      "1. **Increased shortness of breath**: 5 days prior to admission\n",
      "2. **Progressive shortness of breath**: Over the last 2-3 years\n",
      "3. **Dry cough**: Currently present (exact onset not specified)\n",
      "4. **Dyspnea on exertion**: Progressive over several years\n",
      "5. **Osteoarthritis**: Likely a long-standing condition, but exact onset not specified\n",
      "6. **Orthopnea**: Started 2.5 years prior to admission\n",
      "7. **Lower extremity edema**: With multiple episodes of cellulitis (exact onset not specified, but described as several years)\n",
      "8. **Light-headedness upon standing**: Recent symptom (exact onset not specified, but likely related to orthopnea)\n",
      "9. **Chest twinges**: Occasional, nonradiating, sharp, lasting a few seconds on the left (exact onset not specified)\n",
      "10. **Inability to elevate legs**: Several weeks prior to admission\n",
      "\n",
      "Let me know if you need further clarification!\n"
     ]
    }
   ],
   "source": [
    "chat_response = client.chat.completions.create(\n",
    "    model = 'deepseek-ai/DeepSeek-R1-Distill-Qwen-14B',\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Extract clinical 'PROBLEM', 'TEST', 'TREATMENT' from the text and estimate the time of each event happened.\" + text},\n",
    "    ]\n",
    ")\n",
    "print(\"Chat response:\", chat_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c860b5b-0d67-4aa7-8874-ad425f49522c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuhk",
   "language": "python",
   "name": "cuhk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
